{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cuisine Prediction Based on Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for this machine learning project comes from two things I love- food and economics,in that order. Or specifically from this article called [What Are the Defining Ingredients of a Culture’s Cuisine?](http://priceonomics.com/what-are-the-defining-ingredients-of-a-cultures/).  As Dan argues in the article, every cuisine (or most of them) seem to have some specific ingredients that underpin the most if not all dishes from that cusines. See [here](https://tableagent.com/article/an-overview-of-indias-regional-cuisines/), [here](http://www.splendidtable.org/story/the-very-definition-of-mexican-food-is-a-multicultural-cuisine) and [here](http://www.travelchinaguide.com/intro/cuisine_drink/cuisine/) for some examples of what I mean by the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, what interesting from a data perspective is that, having certain defining ingredient(s) simply translates to saying there are certain specific axes or vectors  which a prediction algorithm ought to be able to identify that helps you predict with some accuracy that a dish belongs to a certain cuisine given its ingredients.\n",
    "\n",
    "[Prof.Yong-Yeol “YY” Ahn](http://yongyeol.com/) of Indian University,Bloomington provides such a [dataset](http://yongyeol.com/data/). The dataset accumulated from various recipes from magazines like [Epicurious](http://www.epicurious.com/),[Gourmet](http://www.gourmet.com/) and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the usual tools\n",
    "__author__ = 'satish'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier,Perceptron,PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "#Set seed to replicate results.\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Read Data\n",
    "Using Pandas read csv method to read the data. Since there are no header columns , we read in the data as is and then later split the data using a tab as the delimiter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 col\n",
       "0  Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...\n",
       "1  Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...\n",
       "2  Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...\n",
       "3  Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...\n",
       "4  Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epic_df = pd.read_csv('data/epic_recipes.txt',names=['col'],header=None)\n",
    "epic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the data above, the data is not clearly tabulated to create a useable dataframe. Some amount of data munging would be need to done before the data is in a usable format. First thing we need to do is to split the 'col' column into Cuisine and the corresponding ingredients. The first element when split by '\\t' we get is the cuisine. Further the rest of the columns would be split by '\\t' and then joined by ',' and reset into the dataframe for pandas to parse it correctly. Since there are 3 files in all to parse, we encapsulate the clean up and parsing logic into a simple function that we apply on all data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(filenames):\n",
    "    dfs=[]\n",
    "    for filename in filenames:\n",
    "        epic_df = pd.read_csv(filename,names=['col'],header=None)\n",
    "        epic_df['cuisine']=epic_df['col'].apply(lambda x : x.split('\\t')[0])\n",
    "        epic_df['ingredients'] = epic_df['col'].apply(lambda x:(',').join (x.split('\\t')[1:]))\n",
    "        epic_df.drop('col',inplace=True,axis=1)\n",
    "        dfs.append(epic_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13408 entries, 0 to 13407\n",
      "Data columns (total 2 columns):\n",
      "cuisine        13408 non-null object\n",
      "ingredients    13408 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 314.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41825 entries, 0 to 41824\n",
      "Data columns (total 2 columns):\n",
      "cuisine        41825 non-null object\n",
      "ingredients    41825 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 980.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458 entries, 0 to 2457\n",
      "Data columns (total 2 columns):\n",
      "cuisine        2458 non-null object\n",
      "ingredients    2458 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 57.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinese</td>\n",
       "      <td>onion,beef,starch,sake,soy_sauce,scallion,lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chinese</td>\n",
       "      <td>pork,onion,black_pepper,cayenne,scallion,bean,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese</td>\n",
       "      <td>tomato,vinegar,celery_oil,onion,corn,cayenne,g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinese</td>\n",
       "      <td>wheat,sesame_oil,starch,sake,soy_sauce,cayenne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cuisine                                        ingredients\n",
       "0  chinese  onion,beef,starch,sake,soy_sauce,scallion,lett...\n",
       "1  chinese  pork,onion,black_pepper,cayenne,scallion,bean,...\n",
       "2  chinese  tomato,vinegar,celery_oil,onion,corn,cayenne,g...\n",
       "3  chinese  wheat,sesame_oil,starch,sake,soy_sauce,cayenne..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas= clean_data(['data/epic_recipes.txt','data/allr_recipes.txt','data/menu_recipes.txt'])\n",
    "for data in datas:\n",
    "    print data.info()\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we end up with 3 dataframes each with two columns - cuisine and ingredients,each with the cuisine seperated from rest of the ingredients which are left as comma seperated values in the dataframe. We then join the three seperate dataframes into a single dataframe by concatenating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vinegar,cilantro,mint,olive_oil,cayenne,fish,l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>onion,cayenne,fish,black_pepper,seed,garlic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>garlic,soy_sauce,lime_juice,thai_pepper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cuisine                                        ingredients\n",
       "0  Vietnamese  vinegar,cilantro,mint,olive_oil,cayenne,fish,l...\n",
       "1  Vietnamese        onion,cayenne,fish,black_pepper,seed,garlic\n",
       "2  Vietnamese            garlic,soy_sauce,lime_juice,thai_pepper"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(datas)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Standardization\n",
    "Now that we have a concatenated set of dataframes, lets for a unique data uniqueness issues. Printing the 15 of sorted list of  cuisines reveal issues such as spelling mistakes, duplicate classification (china,chinese) and other errors. We run a simple standardization method on all cuisine column values to fix those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'american',\n",
       " 'asian',\n",
       " 'asian',\n",
       " 'austria',\n",
       " 'bangladesh',\n",
       " 'belgium',\n",
       " 'cajun_creole',\n",
       " 'canada',\n",
       " 'caribbean',\n",
       " 'central_southamerican',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chinese',\n",
       " 'east-african']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([x.lower() for x in df.cuisine.unique().tolist()])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_cuisine_names(cuisine):\n",
    "    return {\n",
    "        'italian':'Italian',\n",
    "        'asian':'Asian',\n",
    "        'mexico':'Mexico',\n",
    "        'japanese':'Japanese',\n",
    "        'chinese':'Chinese',\n",
    "        'China'  :'Chinese', \n",
    "        'korean' : 'Korean',\n",
    "        'Korea'  : 'Korean',\n",
    "        'Japan':'Japanese',\n",
    "        'Korea':'Korean',\n",
    "        'France' :'French',\n",
    "        'India'  :'Indian',\n",
    "        'Italy'  :'Italian',\n",
    "        'Thailand' :'Thai',\n",
    "        'Mexico':'Mexican',\n",
    "        'Scandinavia':'Scandinavian',\n",
    "        'Germany':'German'\n",
    "        \n",
    "    }.get(cuisine,cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.cuisine= df.cuisine.apply(lambda x : standardize_cuisine_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'american',\n",
       " 'asian',\n",
       " 'austria',\n",
       " 'bangladesh',\n",
       " 'belgium',\n",
       " 'cajun_creole',\n",
       " 'canada',\n",
       " 'caribbean',\n",
       " 'central_southamerican',\n",
       " 'chinese',\n",
       " 'east-african',\n",
       " 'east_asian',\n",
       " 'eastern-europe',\n",
       " 'easterneuropean_russian']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([x.lower() for x in df.cuisine.unique().tolist()])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another level of classification would be to move from a simple country specific cuisine to a regional specific cuisine. This will help us move away from issues such eastern_europe and easterneurpoean_russian. Fortunately the map.txt in the dataset already classifies all cuisines to each of the region specific. We read this mapping - country specific cuisine to a particular region mapping into dict to make the look ups simple. The we use the apply function in pandas to apply the lambda function to look up the region for each of the cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_dict = {}\n",
    "with open('data/map.txt') as f:\n",
    "    for line in f:\n",
    "        keys = line.split()\n",
    "        if(len(keys)>1):\n",
    "            (key,val)=keys\n",
    "            map_dict[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cuisine_group']= df.cuisine.apply(lambda x : map_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>cuisine_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vinegar,cilantro,mint,olive_oil,cayenne,fish,l...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>onion,cayenne,fish,black_pepper,seed,garlic</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>garlic,soy_sauce,lime_juice,thai_pepper</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>cilantro,shallot,lime_juice,fish,cayenne,ginge...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>coriander,vinegar,lemon,lime_juice,fish,cayenn...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cuisine                                        ingredients  \\\n",
       "0  Vietnamese  vinegar,cilantro,mint,olive_oil,cayenne,fish,l...   \n",
       "1  Vietnamese        onion,cayenne,fish,black_pepper,seed,garlic   \n",
       "2  Vietnamese            garlic,soy_sauce,lime_juice,thai_pepper   \n",
       "3  Vietnamese  cilantro,shallot,lime_juice,fish,cayenne,ginge...   \n",
       "4  Vietnamese  coriander,vinegar,lemon,lime_juice,fish,cayenn...   \n",
       "\n",
       "    cuisine_group  \n",
       "0  SoutheastAsian  \n",
       "1  SoutheastAsian  \n",
       "2  SoutheastAsian  \n",
       "3  SoutheastAsian  \n",
       "4  SoutheastAsian  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Exploration.\n",
    "Lets look at some patterns in the data before we try our hand aat any of the prediction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57691\n",
      "                    cuisine  ingredients\n",
      "cuisine_group                           \n",
      "African            0.610147     0.610147\n",
      "EastAsian          6.422146     6.422146\n",
      "EasternEuropean    0.660415     0.660415\n",
      "LatinAmerican      5.056248     5.056248\n",
      "MiddleEastern      1.118025     1.118025\n",
      "NorthAmerican     71.976565    71.976565\n",
      "NorthernEuropean   0.433343     0.433343\n",
      "SouthAsian         1.076424     1.076424\n",
      "SoutheastAsian     0.792151     0.792151\n",
      "SouthernEuropean   7.245498     7.245498\n",
      "WesternEuropean    4.609038     4.609038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10a45e110>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFhCAYAAACYmKqMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ddb2dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print len(df.cuisine_group)\n",
    "##Examine the cuisine groups. \n",
    "#print df.cuisine_group.value_counts().apply(lambda x : 100*x/float(x.sum()))\n",
    "print df.groupby('cuisine_group').count().apply(lambda x : 100*x/float(x.sum()))\n",
    "df.groupby('cuisine_group')['cuisine'].count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly no suprise there. As most of the recipes came from American Magazines,North American cuisine is defintely overreprested as compared to the other regions, followed by Southern European and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vectorization\n",
    "The features in this data set are the ingredients of the dish themselves.Each ingredient specific to that dish becomes a feature of that dish and hence the cuisine.Since this is text, we use a count vectorizer to do that. Now in this case since an ingredient will either occur or not in a dish ,this fits the Boolean Occurence model of the Bag of Words model. The appropriate vectorizer to choose in this case would be the Binary Vectorizer..essentially setting the binary property of CountVectorizer to true.\n",
    "Once we vectorize using a binary vectorizer, we build a document term matrix using the ingredients of each recipe as the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] [u'almond', u'angelica', u'anise', u'anise_seed', u'apple', u'apple_brandy', u'apricot', u'armagnac', u'artemisia', u'artichoke', u'asparagus', u'avocado', u'bacon', u'baked_potato', u'balm', u'banana', u'barley', u'bartlett_pear', u'basil', u'bay', u'bean', u'beech', u'beef', u'beef_broth', u'beef_liver', u'beer', u'beet', u'bell_pepper', u'bergamot', u'berry', u'bitter_orange', u'black_bean', u'black_currant', u'black_mustard_seed_oil', u'black_pepper', u'black_raspberry', u'black_sesame_seed', u'black_tea', u'blackberry', u'blackberry_brandy', u'blue_cheese', u'blueberry', u'bone_oil', u'bourbon_whiskey', u'brandy', u'brassica', u'bread', u'broccoli', u'brown_rice', u'brussels_sprout', u'buckwheat', u'butter', u'buttermilk', u'cabbage', u'cabernet_sauvignon_wine', u'cacao', u'camembert_cheese', u'cane_molasses', u'caraway', u'cardamom', u'carnation', u'carob', u'carrot', u'cashew', u'cassava', u'catfish', u'cauliflower', u'caviar', u'cayenne', u'celery', u'celery_oil', u'cereal', u'chamomile', u'champagne_wine', u'chayote', u'cheddar_cheese', u'cheese', u'cherry', u'cherry_brandy', u'chervil', u'chicken', u'chicken_broth', u'chicken_liver', u'chickpea', u'chicory', u'chinese_cabbage', u'chive', u'cider', u'cilantro', u'cinnamon', u'citrus', u'citrus_peel', u'clam', u'clove', u'cocoa', u'coconut', u'coconut_oil', u'cod', u'coffee', u'cognac', u'concord_grape', u'condiment', u'coriander', u'corn', u'corn_flake', u'corn_grit', u'cottage_cheese', u'crab', u'cranberry', u'cream', u'cream_cheese', u'cucumber', u'cumin', u'cured_pork', u'currant', u'date', u'dill', u'durian', u'eel', u'egg', u'egg_noodle', u'elderberry', u'emmental_cheese', u'endive', u'enokidake', u'fennel', u'fenugreek', u'feta_cheese', u'fig', u'fish', u'flower', u'frankfurter', u'fruit', u'galanga', u'gardenia', u'garlic', u'gelatin', u'geranium', u'gin', u'ginger', u'goat_cheese', u'grape', u'grape_brandy', u'grape_juice', u'grapefruit', u'green_bell_pepper', u'green_tea', u'gruyere_cheese', u'guava', u'haddock', u'ham', u'hazelnut', u'herring', u'holy_basil', u'honey', u'hop', u'horseradish', u'huckleberry', u'jamaican_rum', u'japanese_plum', u'jasmine', u'jasmine_tea', u'juniper_berry', u'kaffir_lime', u'kale', u'katsuobushi', u'kelp', u'kidney_bean', u'kiwi', u'kohlrabi', u'kumquat', u'lamb', u'lard', u'laurel', u'lavender', u'leaf', u'leek', u'lemon', u'lemon_juice', u'lemon_peel', u'lemongrass', u'lentil', u'lettuce', u'licorice', u'lilac_flower_oil', u'lima_bean', u'lime', u'lime_juice', u'lime_peel_oil', u'lingonberry', u'litchi', u'liver', u'lobster', u'long_pepper', u'lovage', u'macadamia_nut', u'macaroni', u'mace', u'mackerel', u'malt', u'mandarin', u'mandarin_peel', u'mango', u'maple_syrup', u'marjoram', u'mate', u'matsutake', u'meat', u'melon', u'milk', u'milk_fat', u'mint', u'mozzarella_cheese', u'mung_bean', u'munster_cheese', u'muscat_grape', u'mushroom', u'mussel', u'mustard', u'mutton', u'nectarine', u'nira', u'nut', u'nutmeg', u'oat', u'oatmeal', u'octopus', u'okra', u'olive', u'olive_oil', u'onion', u'orange', u'orange_flower', u'orange_juice', u'orange_peel', u'oregano', u'ouzo', u'oyster', u'palm', u'papaya', u'parmesan_cheese', u'parsley', u'parsnip', u'passion_fruit', u'pea', u'peach', u'peanut', u'peanut_butter', u'peanut_oil', u'pear', u'pear_brandy', u'pecan', u'pelargonium', u'pepper', u'peppermint', u'peppermint_oil', u'pimenta', u'pimento', u'pineapple', u'pistachio', u'plum', u'popcorn', u'porcini', u'pork', u'pork_liver', u'pork_sausage', u'port_wine', u'potato', u'potato_chip', u'prawn', u'prickly_pear', u'provolone_cheese', u'pumpkin', u'quince', u'radish', u'raisin', u'rapeseed', u'raspberry', u'raw_beef', u'red_algae', u'red_bean', u'red_kidney_bean', u'red_wine', u'rhubarb', u'rice', u'roasted_almond', u'roasted_beef', u'roasted_hazelnut', u'roasted_meat', u'roasted_nut', u'roasted_peanut', u'roasted_pecan', u'roasted_pork', u'roasted_sesame_seed', u'romano_cheese', u'root', u'roquefort_cheese', u'rose', u'rosemary', u'rum', u'rutabaga', u'rye_bread', u'rye_flour', u'saffron', u'sage', u'sake', u'salmon', u'salmon_roe', u'sassafras', u'sauerkraut', u'savory', u'scallion', u'scallop', u'sea_algae', u'seaweed', u'seed', u'sesame_oil', u'sesame_seed', u'shallot', u'sheep_cheese', u'shellfish', u'sherry', u'shiitake', u'shrimp', u'smoke', u'smoked_fish', u'smoked_salmon', u'smoked_sausage', u'sour_cherry', u'sour_milk', u'soy_sauce', u'soybean', u'soybean_oil', u'spearmint', u'squash', u'squid', u'star_anise', u'starch', u'strawberry', u'strawberry_jam', u'strawberry_juice', u'sturgeon_caviar', u'sumac', u'sunflower_oil', u'sweet_potato', u'swiss_cheese', u'tabasco_pepper', u'tamarind', u'tangerine', u'tarragon', u'tea', u'tequila', u'thai_pepper', u'thyme', u'tomato', u'tomato_juice', u'truffle', u'tuna', u'turkey', u'turmeric', u'turnip', u'vanilla', u'veal', u'vegetable', u'vegetable_oil', u'vinegar', u'violet', u'walnut', u'wasabi', u'watercress', u'watermelon', u'wheat', u'wheat_bread', u'whiskey', u'white_bread', u'white_wine', u'whole_grain_wheat_flour', u'wine', u'wood', u'yam', u'yeast', u'yogurt', u'zucchini']\n"
     ]
    }
   ],
   "source": [
    "# Create a TFIDF vectorizer\n",
    "count_vect = CountVectorizer(decode_error = 'ignore', binary=True)\n",
    "# Call fit to do our frequency vectorization\n",
    "df_dtm =  count_vect.fit_transform(df.ingredients)\n",
    "print df_dtm.toarray(),count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####TBD - Look for patterns such as what ingredients occur most across all cuisines, across each cuisine, whats most rarest among all cuisines, among each cuisine. Types of meats and other specific ingredients across each cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets fit some models.   - Round 1 - Stick to Standard Text Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['African', 'EastAsian', 'EasternEuropean', 'LatinAmerican',\n",
       "       'MiddleEastern', 'NorthAmerican', 'NorthernEuropean', 'SouthAsian',\n",
       "       'SoutheastAsian', 'SouthernEuropean', 'WesternEuropean'], \n",
       "      dtype='|S16')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Before any thing, lets split the entire data set into training and test. \n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.ingredients, df.cuisine_group, test_size=0.3)\n",
    "unique_cuisine_group = df.cuisine_group.unique().tolist()\n",
    "##For us to be able to use any of the prediction models we need turn the cuisine groups from earlier into numerical labels.\n",
    "## We use the standard Label Encoder to this.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "le.fit(unique_cuisine_group)\n",
    "y_train =  le.transform(y_train)\n",
    "y_test =  le.transform(y_test)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70776519528541715"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Build the Document Term Matrix based on the vectorizer that was created earler. \n",
    "train_simple_dtm = count_vect.transform(X_train)\n",
    "test_simple_dtm = count_vect.transform(X_test)\n",
    "\n",
    "##Given that we have a set of feature vectors that have a binary occurence probability in a certain dish/document, we \n",
    "#look at the naive bayseien model as our initial model. \n",
    "# Create the model\n",
    "bnb = BernoulliNB()\n",
    "# Fit the model to the training data\n",
    "bnb.fit(train_simple_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "bnb.score(test_simple_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a simple Bernoulli Naive Bayes model with no frills is able to predict with about 71.1% accuracy on our test data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up a pipeline for vectorization , transformation and classification\n",
    "classfication with a Multinomial Bayesian classifier while also looking building a TF-IDF Transformer\n",
    "\n",
    "\n",
    "I use a stratified K fold classifier to ensure I have equally weighted sample of different classes in my train and test samples. I do this because as I showed earlier I have lop-sided percentage of various types of my cuisine groups in my data set. \n",
    "Similarly I set up a pipe line to build a document term matrix for each train sample using a binary vectorizer with a TF-IDF document transformer.\n",
    "\n",
    "Similarly I use a simple accuracy score function to measure the accuracy of the prediction. I also set up methods to generate a confusion matrix and plotting of a confusion matrix for on hand later.\n",
    "\n",
    "###QQQ - Is a simple accuracy score a good measure of accuracy in this case, or should I also weight the accuracy based on the class i.e. use f1-score with average = weighted ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from  sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, df,title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df.cuisine_group))\n",
    "    plt.xticks(tick_marks, df.cuisine_group, rotation=45)\n",
    "    plt.yticks(tick_marks, df.cuisine_group)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def compute_cm(y_test,y_pred,df):\n",
    "    cm = confusion_matrix(test_y,predictions)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plot_confusion_matrix(cm,data)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized)\n",
    "    plot_confusion_matrix(cm_normalized,data, title='Normalized confusion matrix')\n",
    "    \n",
    "\n",
    "    \n",
    "def kfold_pipeline_classifier(pipeline,data):\n",
    "    #sss = cross_validation.StratifiedShuffleSplit(le.transform(data.cuisine_group),test_size =0.3,random_state=42)\n",
    "    sss = cross_validation.StratifiedKFold(le.transform(data.cuisine_group),n_folds=6,shuffle=True,random_state=42)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for train_indices, test_indices in sss:\n",
    "        train_text = data.iloc[train_indices]['ingredients'].values\n",
    "        train_y = le.transform(data.iloc[train_indices]['cuisine_group'].values)\n",
    "        test_text = data.iloc[test_indices]['ingredients'].values\n",
    "        test_y = le.transform(data.iloc[test_indices]['cuisine_group'].values)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "        #score = f1_score(test_y, predictions, average='weighted') \n",
    "        score =accuracy_score(test_y,predictions)\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.70491632886394351, 0.70763152422541065, 0.70576003327095027, 0.70782192635739549, 0.7104058272632674, 0.70322580645161292]\n",
      " Mean accuracy with BernoulliNB 0.7066\n"
     ]
    }
   ],
   "source": [
    "bernouli_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',BernoulliNB())])\n",
    "scores = kfold_pipeline_classifier(bernouli_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with BernoulliNB %.4f\" % np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.76967051242074624, 0.77573300062383033, 0.77521314202536906, 0.77366340753068441, 0.77273673257023934, 0.7734651404786681]\n",
      " Mean accuracy with MultinomialNB 0.7734\n"
     ]
    }
   ],
   "source": [
    "multinomial_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',MultinomialNB())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(multinomial_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with MultinomialNB %.4f\" % np.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, lets also look at the commonly used classifiers for text classification before moving to feature engineering or grid searches for optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.061843883172227422, 0.060095653982116866, 0.064150551050114371, 0.060120657374661948, 0.060874089490114462, 0.05442247658688866]\n",
      " Mean accuracy with GaussianNB 0.0603\n"
     ]
    }
   ],
   "source": [
    "guassian_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',GaussianNB())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(guassian_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with GaussianNB %.4f\" % np.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logistic Regression \n",
    "Another workhorse model of text classification ,trying the Logistic regression for multi class set up case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.78318262134913208, 0.78800166354751511, 0.78966521106259102, 0.78884959434158519, 0.78886576482830384, 0.78688865764828309]\n",
      " Mean accuracy with Logistic regression 0.7876\n"
     ]
    }
   ],
   "source": [
    "logistic_regr_pipeline_l2 = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',LogisticRegression(multi_class='ovr',random_state=42,penalty='l2', tol=.01,C=1e5))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(logistic_regr_pipeline_l2,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Logistic regression %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.78339049994803034, 0.78820960698689957, 0.78987315450197548, 0.78812148949448724, 0.78938605619146718, 0.78813735691987508]\n",
      " Mean accuracy with Logistic regression 0.7879\n"
     ]
    }
   ],
   "source": [
    "logistic_regr_pipeline_l1 = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',LogisticRegression(multi_class='ovr',random_state=42,penalty='l1', tol=.01,C=1e5))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(logistic_regr_pipeline_l1,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Logistic regression %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Classifiers.\n",
    "So we achived a 75.5% accuracy on the model with the Multinomial NB and about 76.7% with the Logistic Regression for multi class classification. Lets see if we can do better with a more sophisticated algorithm such a support vector machines which is generally regarded as on of the best text classification algorithms. These are specially effective in high dimensional space as is the case here. Running the standard SVM classifiers to see the prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.75584658559401308, 0.75836972343522557, 0.75629028904138074, 0.75473268150613693, 0.75941727367325706, 0.75796045785639954]\n",
      " Mean accuracy with SGDClassifier 0.7571\n"
     ]
    }
   ],
   "source": [
    "sgd_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(sgd_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with SGDClassifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.76145930776426562, 0.74370971095861926, 0.72894572676232061, 0.74880382775119614, 0.53673257023933407, 0.76035379812695114]\n",
      " Mean accuracy with Linear SVC 0.7133\n"
     ]
    }
   ],
   "source": [
    "linear_svc_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',LinearSVC(penalty='l2',tol=.01,C=1e5,multi_class='ovr'))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(linear_svc_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Linear SVC %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initial GridSearch with MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  36 | elapsed:   11.1s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 13.2581779957s\n",
      "Best score: 0.775276725355\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\ttfidf__norm: l2\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "# Create GridSearchCV\n",
    "pipeline = Pipeline(\n",
    "    [('vec', CountVectorizer(encoding='cp874', decode_error='ignore', binary=True)), ('tfidf', TfidfTransformer(sublinear_tf=True,use_idf=True,)),\n",
    "     ('clf', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in {0}s\".format(time() - t0))\n",
    "print(\"Best score: {0}\".format(grid_search.best_score_))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(list(parameters.keys())):\n",
    "    print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run a few ensemble classifiers before attempting Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.77143748051138139, 0.77282179247244753, 0.77573300062383033, 0.77584772207197839, 0.77398543184183144, 0.77492195629552552]\n",
      " Mean accuracy with Random Forest Classifier 0.7741\n"
     ]
    }
   ],
   "source": [
    "## RandomForest Classifier\n",
    "\n",
    "rf_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',RandomForestClassifier())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(rf_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.75480719259952189, 0.75660220420045743, 0.75816177999584111, 0.76242978988974408, 0.7599375650364204, 0.75962539021852238]\n",
      " Mean accuracy with Random Forest Classifier 0.7586\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',DecisionTreeClassifier(max_depth=10,min_samples_split=1))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(dt_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.77268475210477083, 0.77739654813890624, 0.78145144520690368, 0.7791762013729977, 0.77814776274713837, 0.77294484911550465]\n",
      " Mean accuracy with Random Forest Classifier 0.7770\n"
     ]
    }
   ],
   "source": [
    "## Extra Trees Classifier\n",
    "et_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',ExtraTreesClassifier(n_estimators=10))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(et_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.cuisine_group.isnull()].cuisine.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Benchmarking Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "n_samples: 40383, n_features: 378\n",
      "()\n",
      "Extracting features from the test data using the same vectorizer\n",
      "n_samples: 17308, n_features: 378\n",
      "()\n",
      "Extracting 1 best features by a chi-squared test\n",
      "done in 0.047420s\n",
      "()\n",
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, solver='lsqr', tol=0.01)\n",
      "train time: 0.046s\n",
      "test time:  0.003s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "train time: 0.714s\n",
      "test time:  0.002s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, fit_intercept=True, loss='hinge',\n",
      "              n_iter=50, n_jobs=1, random_state=None, shuffle=True,\n",
      "              verbose=0, warm_start=False)\n",
      "train time: 0.533s\n",
      "test time:  0.002s\n",
      "accuracy:   0.066\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.07      1.00      0.12      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.00      0.00      0.00     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.00      0.07      0.01     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0   111     0     0     0     0     0     0     0     0     0]\n",
      " [    0  1134     0     0     0     0     0     0     0     0     0]\n",
      " [    0    95     0     0     0     0     0     0     0     0     0]\n",
      " [    0   849     0     0     0     0     0     0     0     0     0]\n",
      " [    0   190     0     0     0     0     0     0     0     0     0]\n",
      " [    0 12499     0     0     0     0     0     0     0     0     0]\n",
      " [    0    71     0     0     0     0     0     0     0     0     0]\n",
      " [    0   174     0     0     0     0     0     0     0     0     0]\n",
      " [    0   143     0     0     0     0     0     0     0     0     0]\n",
      " [    0  1243     0     0     0     0     0     0     0     0     0]\n",
      " [    0   799     0     0     0     0     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=10, p=2, weights='uniform')\n",
      "train time: 0.006s\n",
      "test time:  38.867s\n",
      "accuracy:   0.739\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.78      0.34      0.47      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   384     0     0     0   750     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    91     0     0     0 12408     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 1.254s\n",
      "test time:  0.286s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/svm/classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy:   0.737\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.81      0.29      0.43      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.59      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   332     0     0     0   787     0     0    14     0     1]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     0     0     0     0   849     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    63     0     0     0 12432     0     1     2     1     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     0     0     0     0   174     0     0     0     0     0]\n",
      " [    0    10     0     0     0   132     0     0     0     0     1]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.085s\n",
      "test time:  0.003s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.879s\n",
      "test time:  0.001s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.034s\n",
      "test time:  0.001s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.793s\n",
      "test time:  0.001s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 0.272727\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.875s\n",
      "test time:  0.001s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 0.818182\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.017s\n",
      "test time:  0.005s\n",
      "accuracy:   0.029\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.01      1.00      0.01       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.00      0.00      0.00     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.05      0.03      0.03     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[  111     0     0     0     0     0     0     0     0     0     0]\n",
      " [  745   389     0     0     0     0     0     0     0     0     0]\n",
      " [   95     0     0     0     0     0     0     0     0     0     0]\n",
      " [  847     2     0     0     0     0     0     0     0     0     0]\n",
      " [  190     0     0     0     0     0     0     0     0     0     0]\n",
      " [12404    95     0     0     0     0     0     0     0     0     0]\n",
      " [   71     0     0     0     0     0     0     0     0     0     0]\n",
      " [  173     1     0     0     0     0     0     0     0     0     0]\n",
      " [  131    12     0     0     0     0     0     0     0     0     0]\n",
      " [ 1243     0     0     0     0     0     0     0     0     0     0]\n",
      " [  795     4     0     0     0     0     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.015s\n",
      "test time:  0.002s\n",
      "accuracy:   0.722\n",
      "dimensionality: 1\n",
      "density: 0.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.00      0.00      0.00      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.72      1.00      0.84     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.52      0.72      0.61     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1134     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     0     0     0     0   849     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0     0     0     0     0 12499     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     0     0     0     0   174     0     0     0     0     0]\n",
      " [    0     0     0     0     0   143     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     0     0     0     0   799     0     0     0     0     0]]\n",
      "()\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.014s\n",
      "test time:  0.001s\n",
      "accuracy:   0.739\n",
      "dimensionality: 1\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "SoutheastAsian: sesame_oil\n",
      "SouthAsian: sesame_oil\n",
      "SouthernEuropean: sesame_oil\n",
      "MiddleEastern: sesame_oil\n",
      "WesternEuropean: sesame_oil\n",
      "LatinAmerican: sesame_oil\n",
      "NorthAmerican: sesame_oil\n",
      "NorthernEuropean: sesame_oil\n",
      "African: sesame_oil\n",
      "EasternEuropean: sesame_oil\n",
      "EastAsian: sesame_oil\n",
      "()\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection', LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)), ('classification', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 0.702s\n",
      "test time:  0.003s\n",
      "accuracy:   0.739\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  SoutheastAsian       0.00      0.00      0.00       111\n",
      "      SouthAsian       0.77      0.34      0.48      1134\n",
      "SouthernEuropean       0.00      0.00      0.00        95\n",
      "   MiddleEastern       0.00      0.00      0.00       849\n",
      " WesternEuropean       0.00      0.00      0.00       190\n",
      "   LatinAmerican       0.74      0.99      0.85     12499\n",
      "   NorthAmerican       0.00      0.00      0.00        71\n",
      "NorthernEuropean       0.00      0.00      0.00       174\n",
      "         African       0.00      0.00      0.00       143\n",
      " EasternEuropean       0.00      0.00      0.00      1243\n",
      "       EastAsian       0.00      0.00      0.00       799\n",
      "\n",
      "     avg / total       0.58      0.74      0.64     17308\n",
      "\n",
      "confusion matrix:\n",
      "[[    0     0     0     0     0   111     0     0     0     0     0]\n",
      " [    0   389     0     0     0   745     0     0     0     0     0]\n",
      " [    0     0     0     0     0    95     0     0     0     0     0]\n",
      " [    0     2     0     0     0   847     0     0     0     0     0]\n",
      " [    0     0     0     0     0   190     0     0     0     0     0]\n",
      " [    0    95     0     0     0 12404     0     0     0     0     0]\n",
      " [    0     0     0     0     0    71     0     0     0     0     0]\n",
      " [    0     1     0     0     0   173     0     0     0     0     0]\n",
      " [    0    12     0     0     0   131     0     0     0     0     0]\n",
      " [    0     0     0     0     0  1243     0     0     0     0     0]\n",
      " [    0     4     0     0     0   795     0     0     0     0     0]]\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/svm/classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.ingredients, le.transform(df.cuisine_group), test_size=0.3)\n",
    "\n",
    "categories = df.cuisine_group.unique().tolist() \n",
    "\n",
    "use_hashing = False\n",
    "select_chi2= True\n",
    "print_top10 = True\n",
    "print_cm = True\n",
    "print_report = True\n",
    "\n",
    "# split a training set and a test set\n",
    "#y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', non_negative=True\n",
    "                                   )\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "duration = time() - t0\n",
    "#print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(X_test)\n",
    "duration = time() - t0\n",
    "#print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "if use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "          select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, category in enumerate(categories):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\"\n",
    "                      % (category, \", \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(classification_report(y_test, pred,\n",
    "                                            target_names=categories))\n",
    "\n",
    "    if print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAu8AAAIxCAYAAAD9t1YOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcXFWd///XOwFDgETWgKxh33QkoLgAEje+osBvwAUY\nBTLjgjIsjiiOgmO7oTgoKjgiCkYQGNlmZFEWZVglgBBWWRRZlIwGHJWEJYbk8/ujbmIROunupJPO\nTV7Px6MfqTp17rnndiqdd5361OlUFZIkSZKWfsOGegKSJEmS+sfwLkmSJLWE4V2SJElqCcO7JEmS\n1BKGd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSlllJdkny8yR/TvLHJNcnecVQz0uSFtYKQz0BSZIW\nhySjgUuAQ4BzgRHArsCMQTzHsKqaPVjjSVJfXHmXJC2rtgSqqn5YHc9W1ZVVdRdAkvcn+WWSJ5Pc\nk2Rc075NkquT/CnJ3Un2mjNgkolJvpXkx0mmA+OTrJfkgiRTk/wmyeFDc7mSlgeGd0nSsup+YFYT\nuN+SZPU5DyR5J/Bp4MCqGg3sDfwxyYrAxcBlwNrA4cBZSbbsGvcA4HNVtSpwY9N/MrAe8Ebgw0l2\nX/yXJ2l5ZHiXJC2TqmoasAtQwHeAqUl+lGQM8D7g+Kq6ten7YFU9CrwaWKWqvlRVz1XV/9ApvTmg\na+j/rqobm9t/B6xVVZ9v+j8EfBfYf4lcpKTljjXvkqRlVlXdB/wjQJKtgB8AXwM2AB7s5ZD1gN/O\n0/ZI0w6dFwKPdT22MbBekj91tQ0Hrl3kyUtSLwzvkqTlQlXdn+T7wAfoBPTNe+k2BdgwSaqqmraN\ngfu6h+q6/SjwUFV1l9VI0mJj2YwkaZmUZKskH0myfnN/QzrlLzfSKW35aJId0rF5ko2AScDTwNFJ\nVkwyHtgT+M85w85zmpuBaUmOTjIyyfAkL3U7SkmLi+FdkrSsmga8Crip2RnmRuBO4KiqOh/4AnA2\n8CRwIbB6Vc0E9gL2AB4HTqbzodYHmjGLrpX3ZpvIPYHtgd80x5wKjF7sVydpuZS/vSsoSZIkaWnm\nyrskSZLUEoZ3SZIkqSUM75IkSVJLuFWkWi2JH9qQJEmtVFXz7mDVJ8O7Ws8PXbdXT08PPT09Qz0N\nLST//trPv8N28++v3ZIB53bAshlJkiSpNQzvkiRJUksY3iUNmfHjxw/1FLQI/PtrP/8O282/v+WT\nv6RJrZakfA5LkqS2SeIHViVJktpmYT+4qPYYzIVGw7skSdIQ813kZddgvziz5l2SJElqCcO7JEmS\n1BKGd0mSJKklDO+SJElSSxjeJUmSpJZwtxlJkqSlzJLYPtIdbtrJ8C5JkrQUWpzReqh3lp/zwsE9\n7gfOshlJkiT16vjjj2eDDTZg9OjRbL311lx11VXMnj2b4447js0335zRo0fzile8gt/97ncA/Pzn\nP+eVr3wlq622GjvttBM33njj3LHGjx/Psccey84778wqq6zCQw89xH333ceb3/xm1lxzTbbeemvO\nO++8obrU1ohvmajNkpTPYUlSmyV5QQlLksW+8t7X/5/3338/b37zm7n55ptZd911efTRR3nuuee4\n4IILOPPMM7ngggvYYostuOuuu1h//fUB2GyzzTj55JM54IADOPfcczn00EN58MEHWX311Rk/fjwP\nP/wwP/nJT9hqq62YNm0aL33pS/n85z/PgQceyJ133smb3/xmrr32WrbZZpvFePVLVm9/v13tA37r\nwZV3SZIkvcDw4cOZMWMG99xzDzNnzmSjjTZi00035bTTTuMLX/gCW2yxBQAve9nLWGONNbj00kvZ\naqutePe7382wYcPYf//92XrrrbnooouATlidMGEC22yzDcOGDeOyyy5jk0024eCDD2bYsGFsv/32\n7Lvvvq6+98HwLkmSpBfYfPPN+drXvkZPTw/rrLMOBxxwAFOmTOG3v/0tm2222Qv6T5kyhY022uh5\nbRtvvDFTpkyZe3/DDTece/uRRx7hpptuYvXVV5/7dfbZZ/OHP/xh8V3UMsAPrKr1/LCLpGWd5YEa\nKgcccAAHHHAA06ZN45BDDuHjH/84G264Ib/+9a/Zdtttn9d3/fXX58ILL3xe2yOPPMIee+wx9373\n/9kbbbQRu+22G1dcccXivYhljOFd7dcz1BOQpMWoZ6gnoOXVAw88wO9+9zt23nlnRowYwUorrURV\n8b73vY9PfepTbLvttmy22WbcddddbLDBBrz1rW/l8MMP55xzzuGd73wnF1xwAffddx977rnn3DG7\nX4juueee/Ou//is/+MEP2G+//QC4/fbbGTVqFFtvvfUSv962MLxLkiQthYb6feUZM2bwiU98gnvv\nvZcVV1yRnXfemVNPPZUxY8YwY8YMdt99d5544gm22WYb/uu//ov11luPSy65hCOPPJIPfehDbLHF\nFlxyySWsscYac8fsXnlfddVVueKKK/jIRz7CRz7yEWbPns3222/PV7/61aG43NZwtxm1WpJyVUrS\nMq3Hspll3fx2I9Gywd1mJEmSpOWU4V2SJElqCcO7JEmS1BLWvKvVkvgElrTM8//qZZs178u2wa55\nd7cZtZ4/8CRJ0vLCshlJkiSpJQzvkiRJUksssGwmySzgzqbfvcDBwLbAQVV15MKcMMn0qlo1yXrA\n16vqnQszjjRH9y98kCRpSbBkU0NlgR9YTTKtqkY1t38A3FpVJy7SCbvGlBZVEn98SpKWqDC44X1Z\n/8Dqhz70IdZff32OPfbYQe27qM466yzOOOMMLr/88sV6nsH+wOpAwvshwN8B5wIfraq9kvQAmzVf\nawFfrqrvNv0/BrwTGAH8V1X1dI+ZZCxwcVW9LMkEYG9gZDPWf1XVx5v+uwM9zTgPAv9YVU8N9EK1\nbDK8S5KWtCUR3pfEu8r9uYaxY8dy+umn84Y3vGGxz2dxevjhh9l000157rnnGDZsyVaND8luM0lW\nAPYAftLLwy8FXg2sCkxOcinwMmDzqtopyTDgoiS7VtV1CzjNy4Htgb8C9yf5BjADOAZ4Y1U9k+Tj\nwEeAz/Xv8iRJklqqZ+jH7utdgeeee44VVmjP5oXLwjscfb30GJlkMnAL8AhwOp0XnHMU8KOqmlFV\nfwT+B9gJ2B3YvTn2VmBLYPM+zvWzqppWVTOAXwJj6bwo2Bb4eTPWQcBGA7g+SZIkLYQDDzyQRx99\nlL322otRo0Zxwgkn8PDDDzNs2DBOP/10Nt54Y970pjcB8M53vpOXvOQlrLbaauy222788pe/nDvO\nhAkT+NSnPgXA1VdfzQYbbMBXv/pV1llnHdZbbz0mTpy4UH3/+Mc/stdee/HiF7+YnXbaiWOPPZZd\nd92112t53eteB8Bqq63G6NGjmTRpEhMnTnxe/2HDhvGtb32LLbbYgtGjR/Nv//ZvPPjgg7zmNa9h\ntdVWY//992fmzJlz+19yySVsv/32rL766uy8887cddddi/YN76e+Xio9U1Xjuhv68TbOnJc0X6yq\nUwcwlxldt2d1ze3KqvqHAYwjSZKkRXTmmWdy/fXXc9ppp80tm3n44YcBuPbaa7nvvvvmlqC87W1v\nY+LEibzoRS/i6KOP5t3vfjeTJ08GOtmxOz/+4Q9/4Mknn2TKlClcccUVvOMd72CfffbhxS9+8YD6\n/vM//zOjRo3iD3/4Aw899BD/7//9P8aOHdvrtVx33XVssskm/OUvf5k75/vuu+8F/a644gomT57M\no48+yrhx47j++us555xzWGONNXjNa17DOeecw0EHHcTkyZN573vfyyWXXMIrXvEKzjzzTPbee2/u\nv/9+XvSiFy3y935BFrXoJ8D/l2REkjWB8cDNwOXAPyVZBSDJ+knWHuDYBUwCdk6yWTPOKkm2WMQ5\nS5IkaRH09PQwcuRIRowYAXRWzFdZZRVWXHFFPv3pT3PHHXcwbdq0uf27y1VWXHFF/u3f/o3hw4ez\nxx57sOqqq3L//fcPqO+sWbO48MIL+cxnPsNKK63ENttsw8EHHzzfspj+lsscffTRrLrqqmy77ba8\n7GUvY4899mDs2LGMHj2aPfbYY+4LklNPPZVDDjmEV77ylSThoIMOYsSIEUyaNKn/38SF1NfKe29X\nWl3tRWcryf+h84HVz1bV74HfJ9kGuLF59TQdeDfw+DxjVi9j/u3BqieaD7Oek2RE03wM8Ks+5q3l\niBtFSpK0ZG244YZzb8+ePZtPfvKTnH/++Tz++ONzV7afeOIJRo164QaDa6655vM+NLryyiszffr0\nXs8zv76PP/44zz333PPmscEGGyzyda2zzjpzb48cOfJ591daaSWmTp0KwCOPPMIZZ5zBSSedNPfx\nmTNn8r//+7+LPIe+LDC8V9XoXtquAa7parqzqg7upd83gG/Mb8yqepjO7jVU1feB73f12avr9pw6\neqlXy8KHTyRJWhrNr1y6u/2ss87ioosu4mc/+xkbb7wxf/7zn1ljjTWe9//zQHbP6U/ftddemxVW\nWIHf/va3bLFFpyjjt7/97SKNOZB5bbTRRhxzzDF88pOfXORxB2ow9soxOUmSJC2D1llnHR588MEF\n9pk+fTojRoxgjTXW4KmnnnpBoK2qfi+09bfv8OHD2Xfffenp6eGZZ57hvvvu48wzz5xvSF977bUZ\nNmxYn9fS23x6m9v73/9+TjnlFG6++WaqiqeeeopLL710vu8gDKZF2tunqj4zWBORJElSl56hngB8\n4hOf4PDDD+foo4/mU5/6FPvuu+8LAvJBBx3E5Zdfzvrrr8+aa67JZz/7Wb797W/PfXzeD6EuaBV8\nIH1PPvlkJkyYwLrrrsvWW2/NAQccwC9+8Yte+6688socc8wx7Lzzzjz33HP85Cc/6de55n18zv0d\nd9yR73znOxx22GH86le/YuTIkey6667stttu853vYFngL2mSlnZJyuewJKnNlvXfsLqkfPzjH2fq\n1Kl873vfG+qpPM9g/5KmJfsrpiRJkqRBcP/993PnnXdSVdx8882cfvrp7LPPPkM9rcWuPb8SS5Ik\nSWpMmzaNAw44gClTprDOOuvw0Y9+lL333nuop7XYWTajVrNsRpLUdpbNLNssm5EkSZKWU4Z3SZIk\nqSUM75IkSVJLGN4lSZKkljC8S5IkSS1heJckSdJSadSoUTz88MNDPY2liuFdkiRpKZNksX/1x9ix\nY7nqqqsW+XomTpzIrrvuusA+48eP57TTTnte27Rp0xg7duwin39Z4i9pkiRJWiotzr3f+xfel+Qe\n9P19QbG8c+VdkiRJL3DggQfy6KOPstdeezFq1ChOOOEEACZNmsRrX/taVl99dbbffnuuueaaucdM\nnDiRzTbbjNGjR7Ppppty9tlnc9999/HBD36QG2+8kVGjRrHGGmu84FzHHHMM1113HYcddhijRo3i\niCOOAGDYsGH85je/AWDChAkceuihvPWtb2XUqFHsuuuu/P73v+fII49k9dVXZ5tttuH222+fO+aU\nKVN4+9vfzpgxY9h000056aSTFue3a8mpKr/8au1X5yksSVJ79fZ/GVBQi/Grf/9/jh07tn72s5/N\nvf+73/2u1lxzzfrJT35SVVVXXnllrbnmmvXEE0/U9OnTa/To0fXAAw9UVdXvf//7uueee6qqauLE\nibXLLrss8Fzjx4+v00477XltSerBBx+sqqqDDz641lprrbrtttvq2WefrTe84Q218cYb15lnnlmz\nZ8+uY489tl7/+tdXVdWsWbNqhx12qM997nM1c+bM+s1vflObbrppXX755f267sE0v+910z7g7OPK\nuyRJkvrlBz/4AW9961t5y1veAsCb3vQmXvGKV3DppZeShGHDhnHXXXfxzDPPsM4667DtttsCzFlw\n69OC+iVh3333Zdy4cYwYMYJ99tmHVVZZhfe85z0k4V3veheTJ08G4JZbbuGJJ57g2GOPZYUVVmCT\nTTbhfe97H//5n/+5iN+BoWd4V+stygdxJElS/z3yyCOcd955rL766nO/brjhBn7/+9+z8sor88Mf\n/pBTTjmF9dZbjz333JP7779/QOP39f/3mDFj5t5eaaWVnnd/5MiRTJ8+fe48p0yZ8rx5fvGLX2Tq\n1KkDms/SyA+sqv16+tkmSZIGZN4wvdFGG3HggQdy6qmn9tp/9913Z/fdd2fGjBkcc8wxvP/97+fa\na6/t16LaYC68bbjhhmyyySY88MADgzbm0sKVd0mSJPVqnXXW4cEHH5x7/z3veQ8XX3wxV1xxBbNm\nzeLZZ5/l6quv5rHHHmPq1Kn86Ec/4qmnnmLFFVdklVVWYfjw4XPH+d3vfsfMmTP7fa559bf0BmCn\nnXZi1KhRfPnLX+aZZ55h1qxZ3H333fziF7/o9xhLK8O7JEnSUimL8at/PvGJT/D5z3+e1Vdfna9+\n9atssMEG/OhHP+K4445jzJgxbLTRRnzlK1+hqpg9ezYnnngi66+/PmuuuSbXXXcd3/rWtwB44xvf\nyHbbbce66677vFKXbkceeSTnn38+a6yxBh/+8Idf+N2Ypyy2tzLZOfeHDx/OJZdcwu23386mm27K\n2muvzQc+8AGefPLJfl/70ioDeRUjLW2S1PzKZnxuS5LaYEnupa4lb35/v037gGuFXHmXJEmSWsLw\nLkmSJLWEZTNqtSTzfQL73JYktYFlM8u2wS6bcatItZ4/8CRJ0vLCshlJkiSpJQzvkiRJUksssGwm\nySzgTjobgs4CDquqG5fExHqZy3jgqKraK8kEYMeqOjzJIcDTVXVmkonAm4BNq+qvSdYCbqmqTZKM\nBe4F7qNzPU8B/1hVy96v3lrODOZvZJMkqT8Gu2TT/8vUX33VvD9dVeMAkuwOfBEY35+B0zwLa/EU\nJM8ds6q+Pc9jzwH/BJzSy3G/7rqeDwCfBCYshvlpCbLiXZK0JA12zPazWxqIgZTNvBj4vzl3knws\nyc1J7kjS07SNTXJ/ku8DdwG7Jrk3yalJ7k5yeZKVmr7bJ5nUHH9hktWa9quT7NjcXivJQ73MZe6/\nmyQ9SY5q7hbwdeBfkvR1bc+7HkmSJGlp11fAHZlkcpJ7ge8An4O5q/CbV9VOwDhgxyS7NsdsDnyz\nql4KPNrcP7m5/2fg7U2/M4CPVdXL6QT9TzftxcAWU+ft/yhwPXBQL+Ns1lzPr4EPAycO4DySJEnS\nkOorvD9TVeOqahvgLcCZTfvuwO5JJgO3AlvRCekAj1TVzV1jPFRVdza3bwXGJhkNvLiqrmvavw+8\nbhGuo/sdrKJT3vMxXnh9DzbXsznwL8Cpi3BOSZIkaYnq9z7vVTWpKWNZu2n6YlU9L/w2Hwp9ap5D\nZ3TdngWs1Mvw3eH7Of4Wunvr2+v05pnrr5PcDuy3gGMuBr7Xz/ElSZKkIdfvmvckWzf9nwAuB/4p\nySrNY+t3hfo+h6qqJ4E/JdmlaTsQuLq5/TDwiub2O/ozHs8P/3NufwH46AKO2wX4dT/GlyRJkpYK\nfa28j2xKY6ATig9udo+5Msk2wI3NpjLTgPfQe736/O4fDJySZGXgQeAfm/YTgHOb3WAunef46vqz\nt9tz+1TVL5PcSqcmf47NmusJnXcE3rfgy1cbuLmWJElaXsTtidRmSRbTbqSSJEmLTxKqasBrkP6G\nVUmSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqCcO7JEmS1BKG\nd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqCcO7JEmS1BKG\nd0mSJKklVhjqCUiLKsl8H6uqJTgTSZKkxcvwrvbrGWC7JElSS1k2I0mSJLWE4V2SJElqCcO7JEmS\n1BKGd0mSJKklDO+SJElSS8St9NRmSRb4BPb5LUmSlkZJqKr573c9H24VqdYzoEuSpOWFZTOSJElS\nSxjeJUmSpJZYYNlMktnAWVV1YHN/BeB/gUlVtVcfx06vqlWTbAy8tqrOadp3BA6qqiMH5Qp6P/de\nwLZVdfwC+kwAdqyqw5P0AB8DxlbV493zb27PAu4EAswCDquqGxfX/DUwyYDLxSRJ6pWlmFra9VXz\n/hSwXZKVqupZ4M3A74D+PLPn9NkE+AfgHICquhW4deGm2z9VdTFwcV/d5rn/BHAU8K+9PP50VY0D\nSLI78EVg/KLPVIPBH7OSpMHgUpDaoD9lMz8G3tbcPoBOCA9Akp4kR83pmOTuJBvNc/yXgF2TTE7y\n4STjk1zcdfzpSf4nyYNJDu8a6yNJ7mq+jmzaxia5L8n3ktyf5Kwkuye5IckDSV7Z9JuQ5KTm9l5J\nJiW5LcmVScb0co0FnA7sl2S1Pr4fLwb+rx/fN0mSJGlQ9Se8/xDYP8kI4GXATV2Pzbvo2dsi6MeB\n66pqXFV9rZfHtwR2B3YCPp1keFNaM6FpezXw/iTbN/03A04Atga2Avarqp2BjwKf7GUe11XVq6tq\nh+Zajm7a532BPZ1OgP9wL3Mc2bz4uBf4DvD5XvpIkiRJi1WfW0VW1V1JxtJZdb90Ic6xoHehCri0\nqmYCf0wyFVgX2AW4sKqeAUhyIbArcBHwUFXd07TfA/y0GetuYGwv59wwybnNuC8CfrOAuXwDuD3J\nCfM89kxX2cyrgTOAly7ooiVJkqTB1t/dZi6is9o9t2Sm8dw8Y6y0EHP4a9ftWXReUNQ85wl/W02f\n0dU+u+v42fT+YuQk4BtV9XfAIcDI+cwjVfUX4GzgsPlNtqomAWslWWt+fSRJkqTFob/h/XSgZ86K\nd5eHgR0AkuxA58Op85oGjJrPuL2tyhdwHfD3SUYmWQX4+6ZtYT5LMhqY0tye0I95fJVOyO/1XYkk\nWwPDgT8uxFwkSZKkhdZX2UwBVNVjwMldbXNWwS8ADkpyN51a+PvnPRa4A5iV5HZgIjC567Husf52\nYNXkJBOBm5um71TVHU35zoLq7Hsbtwc4L8mfgKuAjXvpM/d2Vf2xKdPprn0fmWRyczt0trp0k5Ol\nhLsDSJKk5UXMoGqzJL6OkiRJrZOEqhrwGqS/YVWSJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxje\nJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxje\nJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqiRWGegLSokoy38eqagnORJIkafEyvKv9\negbYLkmS1FKWzUiSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqCcO7JEmS1BJxKz21WZIFPoF9fkuS\npKVREqpq/vtdz4dbRar1DOiSJGl5YdmMJEmS1BKGd0mSJKklFhjek8xOckLX/Y8m+fTin9YL5vHi\nJB+ap23LJD9O8kCSW5P8MMmYhRz/w0lGLsRxN8ynfWKSty/MXDRwSRbpS5IkqS36Wnn/K7BPkjWb\n+4NSXJxkoLX2qwOHdh2/EnAJ8M2q2rKqdgT+A1h7Iad0JLDyfOY63+9RVe08v4cYpO+V+laL8CVJ\nktQmfYX3mcCpwL/M+0CStZOcn+Tm5uu1TftOSX6e5LYkNyTZsmmfkOSiJD8DrkyycpLTk9zU9N27\n6bdd0zY5ye1JNge+BGzWtH0Z+Afg51V16Zz5VNU1VXVPkuFJ/r2Z0x1JPtCMOz7J1UnOS3Jvkh80\n7UcA6wH/08yNJNOTnJDkduA1ST6S5K7m68iu78H05s8kOTnJfUmuBMYALulKkiRpUPVnBfw/gDub\n0Nzt68CJVXVDko2Ay4BtgXuBXatqVpI3AccB72iOGQe8rKr+nOQ44GdV9U9JVgNuSvJT4BDg61V1\ndrNCvwLwcWC7qhoHkOQrwK3zme97gT9X1U5JRgDXJ7mieWz7Zo7/C9yQ5LVV9Y0k/wKMr6r/a/qt\nDEyqqo8m2RGYAOxE58XOTUmurqo7+Nvi7T7AlsA2wLrAL4HT+vG9lSRJkvqtz/BeVdOSnAEcATzT\n9dCbgG26aoZHJVkZWA04o1kxr3nOcWVV/bm5vTuwV5KPNvdHABsBNwLHJNkAuLCqfp3eC5Pnt7K9\nO/CyJHNeMIwGNqfzLsLNVTUFoFlVHwv8vJcxZgEXNLd3aebxTHPchcDrgDu6+r8OOLs6exb+b5Kr\n5jM3SZIkaaH1t/b8a8BtwPe62gK8qqr+2t0xyX/QWVHfJ8nGwNVdDz81z7j7VtWv5mm7L8kkYE/g\nx0kOAR6ap889wG4LmO9hVXXlPPMaD8zoaprF/K//2frb5uHF818ohBeWS8/bR5IkSRp0/doqsqr+\nBJxLpyRlTnC9gs5qPABJXt7cHA1MaW7/4wKGvXye4+eUxGxSVQ9V1UnAj4CXAU8Co7qOPRt4bZK3\ndh3/uiTbNeMeOudDsc2uNL1+GLXLtGbevbkO+PskI5OsAvx909btWmC/JMOSvAR4fR/nkyRJkgas\nr/DevcL8FWCtrvtHAK9oPhR6D51adYAvA19MchswvGuMeTf4+BywYpI7k9wNfKZpf1eSu5NMBrYD\nzmhq0W9oPjB6fFU9S2dl/vB0toq8B/ggMBX4Lp2a89uS3AV8i84K+4I2GDkVuGzOB1a7+1XVZGAi\ncDMwCfhOU+8+t19V/Rfwq+a836f3UhxJkiRpkcRfLa82S7LIT2D/DUiSpCUtCVU14LLrge63Li11\nDN+SJGl50a+ad0mSJElDz/AuSZIktYThXZIkSWoJw7skSZLUEoZ3SZIkqSUM75IkSVJLGN4lSZKk\nljC8S5IkSS1heJckSZJawvAuSZIktYThXZIkSWoJw7skSZLUEoZ3SZIkqSUM75IkSVJLGN4lSZKk\nljC8S5IkSS2xwlBPQFpUSYZ6CpKkpVBVDfUUpEFneFf79Qz1BCRJS52eoZ6AtHhYNiNJkiS1hOFd\nkiRJagnDuyRJktQShndJkiSpJQzvkiRJUkvEbZTUZkl8AkuSemXG0dIsCVU14P2u3SpSrecPZ0mS\ntLywbEaSJElqCcO7JEmS1BJ9ls0kOQY4AJgFzAYOAW4FPge8A3iq6XpeVR3XHDMLuBNYEXgOOAM4\nsZr6hiR0+YYOAAAgAElEQVQ7AScAY4Cnm/GOAPYDdqyqwwfj4pJcChxQVU8mOQL4YHOuc4Ftq+r4\nwTiPhlYy4HIxSZIWiSWbGioLDO9JXgO8DRhXVTOTrAGMAL5AJ3i/tKr+mmRV4KiuQ5+uqnHNGGsD\nZwOjgZ4k69AJz/tV1U1Nn7cDo4BB/ZdQVW/ruvsh4I1VNaW5f3F/x0myQlU9N5hz0+Dxx6ckaUly\nyUhDqa+ymXWBJ6pqJkBV/R/wF+B9wOFV9demfXpVfaa3AarqceADwGFN0z8DE+cE96bPBVU1tfu4\nJHslmZTktiRXJhnTtO+WZHLzdVuSVZK8JMm1TdtdSXZu+j6cZM0kpwCbApcl+XCSCUlOavqsneT8\nJDc3X69t2nuSnJnkeuD7/f2GSpIkSYtLX+H9CmDDJPcn+WaS1wGbA49W1VN9HDtXVT0EDG8C+HZ0\nSlf6cl1VvbqqdgB+CBzdtB8FHNqs7O8CPEunrOeypu3lwB1zTt05fX0QmAKMr6qv8fzF2q/TKenZ\niU4Z0He7Htuazmr9u/t7rZIkSdLissCymap6KsmOwK7A6+mE6OO6+ySZABwJrAm8pqoe68d5+/OO\n04ZJzqWz+v8i4DdN+w3AiUnOAi6sqseS3AKcnmRF4L+r6o7eh+zVm4BtuuqmRyVZhU7Av6iqZgxg\nLEmSJGmx6XO3maqaXVXXVFUPndKXvekE61Wbxyc2K95/AYb3NkaSTYFZTWnMPcCO/ZjbScA3qurv\n6HxIdmRzvuOB9zb3b0iyVVVdR+cFxmPAxCQH9mP8udMDXlVV45qvDbveVXh6AONIkiRJi9UCw3uS\nLZNs0dU0DrgXOB04OcmIpt9wOqvjvY2xNnAKnTAOcDJwcLPjzJw++zQlNd0r8qPplLoATOjqu1lV\n3VNVXwZuAbZKshHweFV9FzitmecCL63r9hV0drqZM/7L+zhWkiRJGhJ9bRW5KnBSktXobPn4Kzof\nPn2SzlaRdyeZBjwDTORvYXtkksnMs1UkQFVNTbI/cEIT2GcD1wCX0dSoN2P0AOcl+RNwFbBx035k\nktc3x93dHLc/8LEkM4FpwEG9XEvNc3vO/SOAbya5o/l+XAMc2ssxkiRJ0pCK+5SqzZL4BJYkLXHm\nJy2qJFTVgHce7fOXNElLO3+ASpKk5UWfH1iVJEmStHQwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2S\nJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2S\nJElqCcO7JEmS1BKGd0mSJKklDO+SJElSS6ww1BOQFlWSoZ6CJC33qmqopyAtFwzvar+eoZ6AJC3n\neoZ6AtLyw7IZSZIkqSUM75IkSVJLGN4lSZKkljC8S5IkSS1heJckSZJaIm7tpDZL4hNYkpYC5glp\nYJJQVQPe79qtItV6/ochSZKWF5bNSJIkSS1heJckSZJaos+ymSTHAAcAs4DZwCHArcDngHcATzVd\nz6uq45pjZgF3AisCzwFnACdWU9+QZCfgBGAM8HQz3hHAfsCOVXX4YFxckkuBA6rqySRHAB9sznUu\nsG1VHT8Y59HQSgZcLiZJ0iKxZFNDZYHhPclrgLcB46pqZpI1gBHAF+gE75dW1V+TrAoc1XXo01U1\nrhljbeBsYDTQk2QdOuF5v6q6qenzdmAUMKj/EqrqbV13PwS8saqmNPcv7u84SVaoqucGc24aPP74\nlCQtSS4ZaSj1VTazLvBEVc0EqKr/A/4CvA84vKr+2rRPr6rP9DZAVT0OfAA4rGn6Z2DinODe9Lmg\nqqZ2H5dkrySTktyW5MokY5r23ZJMbr5uS7JKkpckubZpuyvJzk3fh5OsmeQUYFPgsiQfTjIhyUlN\nn7WTnJ/k5ubrtU17T5Izk1wPfL+/31BJkiRpcekrvF8BbJjk/iTfTPI6YHPg0ap6qo9j56qqh4Dh\nTQDfjk7pSl+uq6pXV9UOwA+Bo5v2o4BDm5X9XYBn6ZT1XNa0vRy4Y86pO6evDwJTgPFV9TWev1j7\ndTolPTvRKQP6btdjW9NZrX93f69VkiRJWlwWWDZTVU8l2RHYFXg9nRB9XHefJBOAI4E1gddU1WP9\nOG9/3nHaMMm5dFb/XwT8pmm/ATgxyVnAhVX1WJJbgNOTrAj8d1Xd0fuQvXoTsE1X3fSoJKvQCfgX\nVdWMAYwlSZIkLTZ97jZTVbOr6pqq6qFT+rI3nWC9avP4xGbF+y/A8N7GSLIpMKspjbkH2LEfczsJ\n+EZV/R2dD8mObM53PPDe5v4NSbaqquvovMB4DJiY5MB+jD93esCrqmpc87Vh17sKTw9gHEmSJGmx\nWmB4T7Jlki26msYB9wKnAycnGdH0G05ndby3MdYGTqETxgFOBg5udpyZ02efpqSme0V+NJ1SF4AJ\nXX03q6p7qurLwC3AVkk2Ah6vqu8CpzXzXOCldd2+gs5ON3PGf3kfx0qSJElDoq+tIlcFTkqyGp0t\nH39F58OnT9LZKvLuJNOAZ4CJ/C1sj0wymXm2igSoqqlJ9gdOaAL7bOAa4DKaGvVmjB7gvCR/Aq4C\nNm7aj0zy+ua4u5vj9gc+lmQmMA04qJdrqXluz7l/BPDNJHc0349rgEN7OUaSJEkaUnGfUrVZEp/A\nkqQlzvykRZWEqhrwzqN9/pImaWnnD1BJkrS86PMDq5IkSZKWDoZ3SZIkqSUM75IkSVJLGN4lSZKk\nljC8S5IkSS1heJckSZJawvAuSZIktYThXZIkSWoJw7skSZLUEoZ3SZIkqSUM75IkSVJLGN4lSZKk\nljC8S5IkSS1heJckSZJawvAuSZIktYThXZIkSWqJFYZ6AtKiSrLQx1bVIM5EkiRp8TK8q/16lvBx\nkiRJQ8SyGUmSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmSWiJulac2S7JIT2Cf/5IkaSgk\noaoGvN+1W0Wq9QzgkiRpeWHZjCRJktQShndJkiSpJRZYNpNkelWtOk/bIcDTVXXm4pxYkn8CPgwU\nnRcZxwCrAW+pqn/o6rcW8Etg/abpc8C+wDRgBvDZqrpscc5VQysZcLmYJEmLxJJNDZW+at5f8Mys\nqm8vprkAkE4S2xD4JDCuqqYlWRkYA/wR+EqSkVX1THPIO4CLqmpmki8B6wDbNffHALstzvlq6Pnj\nU5K0JLlkpKE04LKZJD1JjmpuX53kS0luSnJ/kl2a9uFJ/j3JzUnuSPKBpn3VJD9NcmuSO5Ps3bSP\nbY7/PnAXMJbOyvlTAFX1dFU9XFXTgGuAvbqmtD9wThPw3wccXlUzm+OmVtV5C/WdkSRJkpYyC1Pz\nXvxtsbOA4VX1KjolLp9u2t8L/LmqdgJ2At6fZCzwDLBPVe0IvAH4Ste4mwPfrKqXAtcDfwAeSnJ6\nkj27+p1DJ7CTZD1gC+Cq5vhHq2r6QlyTJEmStNQbjA+sXtj8eRudFXOA3YGDkkwGJgFr0AnXAb6Y\n5A7gSmC9prQF4JGquhmgqmZX1VvolMQ8AJyYZM4Lgx8DOycZBbwLOL8sPJMkSdJyYDD2eZ/R/Dlr\nnvEOq6oruzsmmQCsBexQVbOSPASs1Dz81LwDV9UtwC1JrgS+B3ymqp5JchmdD6XuB/xL0/3XwEZJ\nRjXlNZIkSdIyZWFX3vv6rMblwKFJVgBIsmVTkz4amNoE99cDG/c6ePKSJDt0NY0DHu66fw7wEWBM\nVU2CTl08cBrw9SQrNuOsneQdA746SZIkaSnU18r7ykl+23X/q82f8ytTmdP+XTolNLc1u8dMBf4e\nOAu4OMmdwC+Ae3s5FmBF4N+bmvZnm+M/2PX4T4GXNOfpdizweeCXSZ6ls5r/qT6uUZIkSWqFWC6u\nNkviE1iStMSZn7SoklBVA955dDBq3qUh5Q9QSZK0vBiM3WYkSZIkLQGGd0mSJKklDO+SJElSSxje\nJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxje\nJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2SJElqiRWGegLSokoy1FOQJC2lqmqopyANKsO7\n2q9nqCcgSVoq9Qz1BKTBZ9mMJEmS1BKGd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmS\nWiLuf6o2S+ITWJI0X+YcLa2SUFUD/mU17vOu1vMHsyRJWl5YNiNJkiS1RJ8r70mOAQ4AZgGzgUOA\nW4HPAe8Anmq6nldVxzXHzALuBFYEngPOAE6sZok0yU7ACcAY4OlmvCOA/YAdq+rwwbi4JJcCB1TV\nk0mOAD7YnOtcYNuqOn4wzqOhlQz4HSdJkhaJ7/pqqCwwvCd5DfA2YFxVzUyyBjAC+AKd4P3Sqvpr\nklWBo7oOfbqqxjVjrA2cDYwGepKsQyc871dVNzV93g6MAgb1X0JVva3r7oeAN1bVlOb+xf0dJ8kK\nVfXcYM5Ng8cfn5KkJcklIw2lvspm1gWeqKqZAFX1f8BfgPcBh1fVX5v26VX1md4GqKrHgQ8AhzVN\n/wxMnBPcmz4XVNXU7uOS7JVkUpLbklyZZEzTvluSyc3XbUlWSfKSJNc2bXcl2bnp+3CSNZOcAmwK\nXJbkw0kmJDmp6bN2kvOT3Nx8vbZp70lyZpLrge/39xsqSZIkLS59hfcrgA2T3J/km0leB2wOPFpV\nT/Vx7FxV9RAwvAng29EpXenLdVX16qraAfghcHTTfhRwaLOyvwvwLJ2ynsuatpcDd8w5def09UFg\nCjC+qr7G8xdrv06npGcnOmVA3+16bGs6q/Xv7u+1SpIkSYvLAstmquqpJDsCuwKvpxOij+vuk2QC\ncCSwJvCaqnqsH+ftzztOGyY5l87q/4uA3zTtNwAnJjkLuLCqHktyC3B6khWB/66qO3ofsldvArbp\nqpselWQVOgH/oqqaMYCxJEmSpMWmz91mqmp2VV1TVT10Sl/2phOsV20en9iseP8FGN7bGEk2BWY1\npTH3ADv2Y24nAd+oqr+j8yHZkc35jgfe29y/IclWVXUdnRcYjwETkxzYj/HnTg94VVWNa7427HpX\n4ekBjCNJkiQtVgsM70m2TLJFV9M44F7gdODkJCOafsPprI73NsbawCl0wjjAycDBzY4zc/rs05TU\ndK/Ij6ZT6gIwoavvZlV1T1V9GbgF2CrJRsDjVfVd4LRmngu8tK7bV9DZ6WbO+C/v41hJkiRpSPS1\nVeSqwElJVqOz5eOv6Hz49Ek6W0XenWQa8Awwkb+F7ZFJJjPPVpEAVTU1yf7ACU1gnw1cA1xGU6Pe\njNEDnJfkT8BVwMZN+5FJXt8cd3dz3P7Ax5LMBKYBB/VyLTXP7Tn3jwC+meSO5vtxDXBoL8dIkiRJ\nQyruU6o2S+ITWJK0xJmftKiSUFUD3nm0z1/SJC3t/AEqSZKWF31+YFWSJEnS0sHwLkmSJLWE4V2S\nJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWE4V2S\nJElqCcO7JEmS1BKGd0mSJKklDO+SJElSSxjeJUmSpJYwvEuSJEktscJQT0BaVEmGegq9qqqhnoIk\nSVrGGN7Vfj1DPYFe9Az1BCRJ0rLIshlJkiSpJQzvkiRJUksY3iVJkqSWMLxLkiRJLWF4lyRJklrC\n8C5JkiS1RNyLWm2WZKl9AvtvS5IkzU8SqmrAv6zGfd7VeoZkSZK0vLBsRpIkSWqJBa68J5leVavO\n03YI8HRVnbk4J5bkn4APA0XnRcYxwGrAW6rqH7r6rQX8Eli/afocsC8wDZgBfLaqLlucc9XQSgb8\njpMkSYvEd301VPoqm3nBM7Oqvr2Y5gJAOklsQ+CTwLiqmpZkZWAM8EfgK0lGVtUzzSHvAC6qqplJ\nvgSsA2zX3B8D7LY456uh549PSdKS5JKRhtKAy2aS9CQ5qrl9dZIvJbkpyf1Jdmnahyf59yQ3J7kj\nyQea9lWT/DTJrUnuTLJ30z62Of77wF3AWDor508BVNXTVfVwVU0DrgH26prS/sA5TcB/H3B4Vc1s\njptaVect1HdGkiRJWsosTM178bfFzgKGV9Wr6JS4fLppfy/w56raCdgJeH+SscAzwD5VtSPwBuAr\nXeNuDnyzql4KXA/8AXgoyelJ9uzqdw6dwE6S9YAtgKua4x+tqukLcU2SJEnSUm8wPrB6YfPnbXRW\nzAF2Bw5KMhmYBKxBJ1wH+GKSO4ArgfWa0haAR6rqZoCqml1Vb6FTEvMAcGKSOS8MfgzsnGQU8C7g\n/LLwTJIkScuBwdgqckbz56x5xjusqq7s7phkArAWsENVzUryELBS8/BT8w5cVbcAtyS5Evge8Jmq\neibJZXQ+lLof8C9N918DGyUZ1ZTXSJIkScuUhV157+uzGpcDhyZZASDJlk1N+mhgahPcXw9s3Ovg\nyUuS7NDVNA54uOv+OcBHgDFVNQk6dfHAacDXk6zYjLN2kncM+OokSZKkpVBfK+8rJ/lt1/2vNn/O\nr0xlTvt36ZTQ3NbsHjMV+HvgLODiJHcCvwDu7eVYgBWBf29q2p9tjv9g1+M/BV7SnKfbscDngV8m\neZbOav6n+rhGSZIkqRViubjaLIlPYEnSEmd+0qJKQlUNeOfRwah5l4aUP0AlSdLyYjB2m5EkSZK0\nBBjeJUmSpJYwvEuSJEktYXiXJEmSWsLwLkmSJLWEu82o9Tq/SuBv3H1GkiQtqwzvWgZ0h/UBb5cq\nSZLUGpbNSJIkSS1heJckSZJawvAuSZIktYThXZIkSWoJw7skSZLUEu42o2WAO8xIkqTlg+Fdree+\n7pIkaXlh2YwkSZLUEoZ3SZIkqSUsm1HrJda8S5Kk5YPhXe3XM9QTkCRJGqCehTvMshlJkiSpJQzv\nkiRJUksY3iVJkqSWMLxLkiRJLWF4lyRJklrC8C5JkiS1RPzV8mqzJD6BJUlSK1XVgH9Zjfu8q/V8\nASpJktpmYX/JpGUzkiRJUksscOU9ySzgTmA48GvgoKqavqgnTTIB2LGqDh+EsR4GngRmNU0fqqpJ\nizpuL+d5ObBeVf2kq20P4LPAysAM4Kqq+miSHmBaVX1lkM59Q1Xt3Nz+d2AP4MfAg8DTVXXmYJyn\nrRb2laskSQvDd3w1lPoqm3m6qsYBJJkIHAIMSiAdRAWMr6r/G8hBSYZX1ay+e841DtgR+Elz/EuB\nk4C3VtUDSYYB7++a06CZE9wb7wdWr4X4ybEQ19wK/giVJC0pLhdpqA2kbOZGYDOAJDsl+XmS25Lc\nkGTLpn1CkguT/CTJA0mOn3Nwkn9Mcn+Sm4DXdrWPTXJVkjuS/DTJhk37xCT/keTGJA8mGZ/k+0l+\nmeR788ztef+W+hjzlCSTgOOTbNbM9RdJrk2yVdPvnUnuSnJ7kquTrEhnhX2/JJOTvAs4Gvh8VT0A\nUFWzq+rb837Tkrw/yc3NWOcnGdnLOa5p2rZLclNzjjuSzPl+T2/+vAhYFbgtybuS9CQ5qnlsftfy\nvGsewN+3JEmSljZVNd8vOqUf0CmbuQA4tLk/Chje3H4TcH5zewKdUo5RwAjgYWB94CXAI8CawP/f\n3p3HWVLVdx//fAGXYWAeJBoTFR0EHhQCZiSCggaQhLjEHTARUQwSxAeXCBoTNYwxi0sSFQWJiIAa\nQVASMBoEl2GXgRmYGRYXFIzGJZiAAg6I8Hv+qNNwaXq6b/f0dHfNfN6vV71u3VOnTv3qVi+/e+65\npx4EXAQc0/b5PHBQW38V8K9t/WTg0239+XRDY3akS9SvAHZu226kG9pzJXDpEG2ezX2z7HwF2Lat\n7wZ8pa2vBH6zrS9oj68cibk9XwbstIbX7WjgyLa+5UD5u4AjxjnGMcDL2vomwEMHr8MY60cDb5rg\nXO53zuvbAlS5uLi4uLjM0NKlTtLaaz9LTHaZaNjMvCRXtgT8RuD4Vr4F8Ikk29L9IA+285WquhUg\nybXAQuARwJKq+p9W/hlgu1b/qcAL2/qngPe29aJLwgGuBn5cVde0/a9p7a5s9faq+w+bGa/NM6qq\nkmwGPA04Y2DM9IPb48XAKUlOB85sZWFqn5btlORvgP9D12t+zjjHuBR4W5LHAGdW1fXDHCDJfLpP\nM8Y6l3vPeQqxS5IkaQ6ZaNjM6urGvD8OuAN4QSt/F12SvhPwPGDewD53DqzfTZfYj04cRyfBa0qK\nf9ke7xnV7j1MPF5/TW3+oj1uBNxSVYsGlh0Bqupw4O3AVsCyJFuO0c41wO+Mc/yRcz6Z7hOLnYF3\n0l6rsY5RVafSvZ6rgS8m2XuCcxyxEXDzWOcy6pwlSZLUY0ONea+q1cDrgb9N17W7APhh2/yqiXYH\nLgP2TLJlGz++/8D2S4A/ausHAhcMGft4Jmyzqn4O3JBkP4B0dm7r21TV0qo6GrgJeAzdsJ3NB5p4\nH/CXSbZr+2yU5LC2bbCXfjPgx+28Xz6y81jHSLI1cGNVfQg4C9hpiHNN+6RjzHORJEnS+mOi5P3e\nHvOquopuusgD6Iah/H2S5XTj4Wug/gOGZ1TVj4HFdMNCLqLrtR7xOuBVSVbQJdpvGOv4Y7U7jmHb\nPBA4JMlVdENznt/K35tkZZJVwMVVtRL4GrBD+zLp/lW1CngjcGobHrQK2HrgGCPHeQfdm5eLgOsG\nysc6xgHAqjZUaUfgE0O8DiPP13QuY+0jSZKkHopDodVnSfwBliTNKHMnTYckVNWkv0850bhxac7z\nj6gkSdpQTGaed0mSJEmzyORdkiRJ6gmTd0mSJKknTN4lSZKknjB5lyRJknrC2WbUe919wyRJktZ/\nJu9aDzhVpCRJ6pupdT46bEaSJEnqCZN3SZIkqSdM3iVJkqSeMHmXJEmSesLkXZIkSeoJZ5vResCp\nIiVJ0obB5F29V+VUkZIkqV+mep8ah81IkiRJPWHyLkmSJPWEybt6L8mUP3qSJEnqE5N39d/i2Q5A\nkiRpZpi8S5IkST1h8i5JkiT1hMm7JEmS1BMm75IkSVJPmLxLkiRJPWHyLkmSJPVEvLW8+izJvT/A\n/ixLkqS+SEJVTfpGNZusi2CkmWTSLkmSNhQOm5EkSZJ6wp539V4y6U+cJElaK37qq9kybs97ktsG\n1p+T5JtJHptkcZLbkzxirLrjtPeFJAsmqLMkyS5jlB+c5EMTHWMqkhyV5LokVyZZmuSg8WKZ4jF2\nSfLBtv6QJF9OsjzJAUlOSPLE6TjOhqhcXFxcXFxmcJFm00Q97wWQZB/gg8C+VfWfrafzp8CRwFsH\n647bWNVzh4hpTe1M+fclLeAa421yktcA+wBPqarbkmwOvGjgmNPye1pVy4Bl7emiFs6T2/PTJ9NW\nko2q6p7piEuSJEn9MeGY9yS/C3wUeG5V3dCKC/g48NIkW4yxz8uTXNZ6so9PslErvzHJlm39HUm+\nkeTCJJ9OcuRAE/u3/b+Z5OkD5Vsl+VqSbyX5q4HjvSnJqra8oZUtbPufAqxq+57c6qwcqQf8BXB4\nVd0GUFW3VtUnxjin45JcnuTqJIsHyt+d5JokK5K8t5Xt345zVZIlrWyvJJ9vn1Z8CnhK63l//GAP\nf5J9k1ySZFmS05PMH3jt3p1kGbDfRNdNkiRJ65+Jet4fCvwrsGdVfWvUttvoEvg3AotHCtvwjwOA\n3avq7iTHAQcCn+S+nvynAC8GdgYeDCwHrhhoe+Oq2i3Js4Gjgd8HAuwK7AisBi5P8oVW/+C2bSPg\nsiTnA7cA2wIHVdXSlhw/qqp2ajEsaEN4Nq+qGyd4HQDeVlU3J9kY+HKSnYAfAi+sqieMtNnqvoPu\nU4ofjR4mVFU3JTkEOKqqntf2K6CSPBx4G7BPVa1O8ufAm4B3tdfup1U1LcN4JEmS1D8T9bz/ErgY\nePUY2wo4Bnhlks0GyvcBdgGuSHIl8Exg64HtAfYA/q2qftl6vD8/qu0z2+NyYOFA+blVdXNV3dHq\nPL21dWZVra6q21v5M1p836uqpW3f7wCPT3JMkj8Abp3g3Ed7aev1Xk73BuKJdG8Q7khyYpIX0b2p\ngO41OyXJqxn7DdJY37AM8FRgB+CS9tq9AnjsQJ3PTDJmSZIkrUcmSt7voetF3zXJX4zalqr6GfBp\n4IhR206pqkVteUJV/fWo7cX9E9jRyeyd7fFu1vzpQLhvPProtkbKb7/3gFW30PX0LwFeA3ysqn4O\n3JZk8M3FAw/UbT8SeGZVPQn4AjCvqu6m6/H/LPCHwDntWIcDbwe2ApaNDBUa0nkDr92OVXXowLbb\n17iXJEmS1nsTjnlvvdzPBQ5M8idjVPkn4DDuS7K/Cuw3MhNNki2TDPYeF13P9PParCubtfaH8ftJ\nHpZkHvAC4CLgQuCFSea18eEvbGX3e0OQ5NeATarqTLphLSNfFv174Nj2RVWSbDYy28yABXSJ88+T\nPBJ4Nt0wl/nAFlX1H3TDW57U2timqpZW1dHATcBjhji3Ar4O7JFkm9bO/CTbDfnaSJIkaT031Gwz\nbaz3s4ALktw0atv/JDmTbuw7VXVtkrcD57Yvqt4FvBb4z3sbrboiydnASuAndF8o/dl4MbTHpcDn\n6JLhT1bVcoAkJ7dtACdU1YokC7n/TDGPBk4a+fIsbZacqvpIewNxeZK7Wrz/cL8AuvauBL4BfJ/u\nTQPA5sBZSR5K92bhz1r5e1vSHeDLVbUyyZ6jzuUBs9hU1U+THAycmuQhrfhtwLfX8NqIsccgSZIk\nrY8yWzcZSDK/qm5PsilwPnBoVV01K8Got5KMNQOoJEnSnJaEqpp0H+Rs3mH1o0l2oJvR5mQTd0mS\nJGl8s9bzLk0He94lSVIfTbXnfcIvrEqSJEmaG0zeJUmSpJ4weZckSZJ6wuRdkiRJ6gmTd0mSJKkn\nTN4lSZKknjB5lyRJknrC5F2SJEnqCZN3SZIkqSdM3iVJkqSeMHmXJEmSesLkXZIkSeoJk3dJkiSp\nJ0XD/EgAABITSURBVEzeJUmSpJ7YZLYDkNZWktkOQdIkVdVshyBJvWTyrv5bPNsBSJqUxbMdgCT1\nl8NmJEmSpJ4weZckSZJ6wuRdkiRJ6gmTd0mSJKknTN4lSZKknjB5lyRJknoizrWrPkviD7DUQ/7v\nkbShS0JVTfpmNc7zrt4zCZAkSRsKh81IkiRJPWHPu3ovmfQnTuuUnwRIkqR1Zdye9yR3J7kyyaok\npyeZNx0HTfKFJAvWYv8XJrknyfbTEc90WptzS/IbSU5Lcn2SK1pb2yVZmGTVNMb4ziT7tPVnJLkm\nyfIkj0pyxnQdZ6bUHFokSZLWpXG/sJrk1qravK1/ClhWVe+fqeDWJMlngHnA8qpaPE1tblJVv5qO\ntqZ4/ACXACdV1Udb2c7AAuAHwOeraqd1cNzjgQur6l+msO+svmYthjnVzx3seZckSROb6hdWJzPm\n/UJg2yR/mOTrraf2vCS/3gLYs/XSX9m2zU/ym0kuGOi936PVvTHJryV5d5LXDpzE4iRHtvU3J1ma\nZEWSxQN1NgN2A44AXjpQniTHJbkuybmt1/olbdtzWvkVSY5J8vmB430yyUXAKUkenuSz7bhLk+w+\ng+e2N/DLkcQdoKpWVtVFgxeh9cJfkGRZW57Wyh8QT5KNkpzcnq9M8oZW9+QkL0lyCLA/8K72Ojwu\nydWtzsZJ3jcQ55+28r2SXJjkLOCaSfz8SJIkaS0NNeY9ySbAc4AvAhdV1VNb+auBtwBHAUcCr62q\nS5NsCtwJHAacU1V/l2QjYNPW5Mgog9OADwDHtfL9gX2T7AtsW1W7tv3OSvKMqroQeEFr8z+T3JTk\nyVW1HHgJ8LiqemKSRwLXAScmeShwPPCMqvpekk9z/xEOTwCeXlV3tm3vr6qLkzwWOAfYYSbODdgR\nWDbE5fgJ8Pst3u2ATwNPAV42EE+A+cAi4FEjPfa5bzhPAVVVJyZ5Ol2v/plJFg68NocAt7Q4HwJc\nlOTctm0RsGNVfW+IeCVJkjRNJkre5yW5sq1fAJwIPDHJ6cBvAA8Gvtu2Xwy8P8m/AGdW1X8luRz4\neJIHAf9WVSsGG6+qq5L8epLfBH4duLnt92d0ie7IsecD29L1/v8xMDJ054z2fDmwB3B6a/cnSb7W\n6jwB+O5Aonkq8KcjIQBnV9Wd7fnvtfMbCXHzJPNn6NyG9WDgw0meBNwNbNfKl46OJ8l3gMcnOQb4\nAnDumC12oz1G2xfYKcl+7fmCFuevgKUm7pIkSTNvouR9dVUtGixI8iHgH6rq35PsCSwGqKr3JPl3\n4LnAxUn+oKoubL3KfwicnOSfquqTo45xBrAf3ZuB0wbK/35wCEk79pZ0w0t+K93NeTYG7gHePFJl\njHMYPQB5dJ1fjNq2W1X9clSdmTi3Z7a6E/kz4EdVdVCSjYE7ANYUT0vy/wB4DXAAXY/6sI6oqvNG\nxbkXcPsk2pAkSdI0mco87wuAH7b1g0cKk2xTVddU1XuBy4Ht29CTm6rqY3S99otGNwZ8hq73fD+6\nZBfgS8CftF5vkjw6ySNanU9U1cKq2rqqHgvc2JLWi4GXpPNIYK/W1jfpep8f156/lPsS+tGJ/LnA\n6wfO6bdn6tyq6qvAQ5IcOnD8nduwlkELgB+39VfQvYFhVDwfA56c5NeAjavqTOAda4hxTb4EvLYN\nmSLJ/21DhiRJkjRLJup5H2vajMXAGUluBr4KjCTFb0iyN11P+NV048X/CHhzkruAW+mSzfu1W1XX\npvsS6g+q6iet7LwkTwQubUNYbgUOau29e1Q8n2vlRwD7ANcC36cbSvOzqroj3RdHz0lyO13yfc9A\nHIPn+Hrg2CQr2mtzPvDaGTi3lwM3AS8CPpDkz+l61G8A3jiq3eOAzyV5RYvjtla+N3DUqHgeDZzU\nxtYDvJWx1RjrHwMWAsvbGPr/bvHNuVkR59Ys75IkSevOuFNF9k2S+VV1e+txvgzYvar+e6S81TkW\n+FZVfXBWg9W0SFLr08+wJEnaMGSKU0Wub3dY/fckW9B9qfOvq+q/W/mhSV7ZypcD/zxbAUqSJElT\ntV71vGvDY8+7JEnqo6n2vE/lC6uSJEmSZoHJuyRJktQTJu+SJElST5i8S5IkST1h8i5JkiT1hMm7\nJEmS1BMm75IkSVJPmLxLkiRJPWHyLkmSJPWEybskSZLUEybvkiRJUk+YvEuSJEk9YfIuSZIk9YTJ\nuyRJktQTm8x2ANLaSjLbIUjSrKiq2Q5B0gwzeVf/LZ7tACRpFiye7QAkzQaHzUiSJEk9YfIuSZIk\n9YTJuyRJktQTJu+SJElST5i8S5IkST1h8i5JkiT1RJwjVn2WxB9gSRss/4dL/ZWEqpr0zWqc5129\n5z8vSZK0oXDYjCRJktQT9ryr95JJf+IkSdJa8VNfzZZxk/ckdwMrW73rgFdW1eqZCGwghhcA36qq\n62byuOoP/3xKkmaSXUaaTRMNm/lFVS2qqp2AXwKvGabRJNPZo/8iYIc1HGfjaTyOJEmSNKdNZsz7\nRcC2STZN8vEklyVZnuT5AEkOTnJ2kq8A5yWZn+SkJCuTrEjy4lZv3ySXJFmW5PQk81v5jUne0+pf\nlmSbJLsDzwPe1471+CRLkrw/yeXAG5Ls07atTHJikgcPtLe4HWdlku2n84WTJEmSZtpQyXvrSX8W\n3RCatwNfqardgGfSJdabtqqLgJdU1d7AXwE3V9XOVfUk4KtJHg68DdinqnYBlgFvavsWcEtV7Qx8\nGPhAVV0CnA0cVVVPrqrvtnoPqqqnAMcBJwEHtP02AQ4faO+mdpyPAEdN5QWSJEmS5oqJkvd5Sa4E\nLge+B3wc2Bd4ayv/GvAQ4LF0yfJ5VXVL23cf4NiRhlr5U+mGwFzS9n9F23fEqe3xNOBpA+Wjh5d9\npj1uD9xQVde356cAvztQ78z2uBxYOMG5SpIkSXPaRGPTV1fVosGCNrPHi6vq26PKdwNuH7X/WN/p\nOK+qXjZEbLWGdcY4zuDxBuve2R7vxpl1JEmS1HNTmef9S8DrR54kGUnuRyfq5wH/b6DeFsDXgT2S\nbNPK5ifZbmCflw48XtLWbwUWjGp75FjfBBaOtAccBJw/2ROSJEmS+mCi3uixZuF7F/CBJCvpkv/v\nAs9vdQfr/w1wbJJVdD3fi6vq35IcDJya5CGt3tuAkV78hyVZAdwB/HErOw04IcnrgP0H46qqO5K8\nCjijjctfChw/RuyjY9N6xCm7JEnShiJz5SYDSW4Adqmq/53tWNQfSWqu/AxLkiQNKwlVNek+yKkM\nm1lXzMAkSZKkccyZnndpKux5lyRJfbQ+9LxLkiRJGofJuyRJktQTJu+SJElST5i8S5IkST1h8i5J\nkiT1hMm7JEmS1BMm75IkSVJPmLxLkiRJPWHyLkmSJPWEybskSZLUEybvkiRJUk+YvEuSJEk9YfIu\nSZIk9YTJuyRJktQTm8x2ANLaSjKt7VXVtLYnSZI0XUze1X+L52hbkiRJ08xhM5IkSVJPmLxLkiRJ\nPWHyLkmSJPWEybskSZLUEybvkiRJUk+YvEuSJEk9Eee0Vp8lmfYfYH8nJEnSupaEqpr0zWqc5129\nZ7ItSZI2FA6bkSRJknrCnnf1XjLpT5wkSVorfuqr2TJu8p7kbmAlsDFwPfCKqrotyaOAD1bV/mPs\nswQ4sqqWTSWgJM8G/hrYFLgT+GpVHZVkMXBrVf3jVNod4zgXV9Uebf19wLOBLwLfAX5RVZ+cjuNo\n3fPPpyRpJtllpNk0Uc/7L6pqEUCSk4HDgH+sqh8CD0jcm2KK+VSS3wI+BDynqr6VZCPg0IF2p81I\n4t4cCjyspvA2OsnGVXX39EUmSZIkjW0yY94vBbYBSLIwyaq2Pi/JaUmuTXImMG9khySHJPlmksuS\nnJDkQ638EUk+m2RpW3Zvu7wF+Juq+hZAVd1TVf88OpAkh7b9rmrtzGvl+ydZ1crPb2U7tuNfmWRF\nkpFzuK09ng1sBixPckCSxUmObNu2SfIfSa5IckGS7Vv5yUmOT/J14D2TeA0lSZKkKRsqeU+yMbAv\ncPUYmw8HbquqHYCjgV3aPo8C3g7sBuwBbM99vecfBN5fVbsC+wEfa+U7AsMMt/lcVe1aVb8NXAcc\n0srfAezbyp/Xyg6jG+KzqMX2X628AKrq+cDqqlpUVadz/08OPgq8rqp+B3gzcNxADI8CnlZVRw0R\nr6QxLJntALRWlsx2AFprS2Y7AK2VJUuWzHYImgUTJe/zklwJ/AjYCjh+jDrPAD4FUFWr6MbIB9gV\nOL+qbqmqXwFncN8wsd8DPtzaPgvYPMn8ScS9U5ILk6wEDgR2aOUXA6ckeTX3DQm6FPjLJG8BFlbV\nHcMcoMWzO3BGi/N44Dfa5gLOmMowG0n3WTLbAWitLJntALTWlsx2AForJu8bpomS99Wtx/pxwB3A\nC9ZQb6zvboxObDNQFmC31tu9qKq2qqrbgWuA3xknnpH9TwZeW1U7A++kDdWpqsPpevu3ApYl2bKq\nTqXrhV8NfDHJ3uO0P2gj4OaBGBdV1Y4D238xZDuSJEnStBhq2ExVrQZeD/xtHjgv3wXAy+DeL5zu\nTJdkXw7smWSLJJsALxnY59zWHm2/326r76PrJd+ulW+U5LCRatz3JmEz4MdJHgS8fKCdbapqaVUd\nDdwEPCbJ1sCNVfUhul7+nYY45VTVrcANSfZrbSfJzkPsK0mSJK0TE802c2/veVVdleR64ADg6wPb\nPgKclORauvHnV7T6P0zyd8BS4H+BbwA/b/u8Hjg2yYoWw/l0PemrkrwRODXJpu0Ynx+IZeSY7wAu\no0vQL6NL5gHe2xL/AF+uqpVJ/hw4KMlddMN//nb0ufHATwlGnh8IfCTJ24EHAafSDQsaax/NEqfs\n6rd3znYAWitev/7zGk7NXLnHyDvf6RXc0GRdDttOMr+qbm8972cCJ1bVWevsgJIkSdJ6bDJTRU7F\n4vZlz1XAd03cJUmSpKlbpz3vkiRJkqbPuu55lyRJkjRNTN415yV5VpJvJPl2+wLyWHWOadtXJFk0\n0zFqfBNdwyQHtmu3MsnFzuw0twzzO9jqPSXJr5K8eCbj0/iG/Bu6V7sT+dVJlsxwiJrAEH9DH57k\nnHaH+auTHDwLYWoMST6e5CdJVo1TZ1I5jMm75rR2d98PA8+iuxnXHyd54qg6zwG2rartgD+lmwFJ\nc8Qw1xD4LvC77d4N76K7u7HmgCGv30i99wDn4CRQc8aQf0O3AI4FnldVv0V353PNEUP+Dh4BXNnu\nML8X8I9tshDNvpPort2YppLDmLxrrtsVuL6qbqyqu4DTeODNwp4PnAJQVZcBWyR55MyGqXFMeA2r\n6tKq+ll7ehnwmBmOUWs2zO8gwOuAz9JN4au5Y5jr9zLgc1X1A4Cq+ukMx6jxDXMNfwQsaOsLgP9p\nd7fXLKuqC4Gbx6ky6RzG5F1z3aOB7w88/0Erm6iOyd/cMcw1HHQI8MV1GpEmY8Lrl+TRdMnESI+R\nMyHMHcP8/m0HbJnka0muSHLQjEWnYQxzDU8AdkzyQ2AF8IYZik1rb9I5jB+paK4bNgkY/TG9ycPc\nMfS1SLI38CfAHusuHE3SMNfvA8Bbq6raXbgdNjN3DHP9HgQ8GdgH2BS4NMnXq+rb6zQyDWuYa/iX\nwFVVtVeSbYDzkjyp3S1ec9+kchiTd811/wVsNfB8K7p3pePVeUwr09wwzDWkfUn1BOBZVTXeR4ya\nWcNcv12A09odJx8OPDvJXVV19syEqHEMc/2+D/y0qlYDq5NcADwJMHmfG4a5hrvT7iBfVd9JcgOw\nPe2u95rTJp3DOGxGc90VwHZJFiZ5MPBSYHRCcDbwCoAkTwVuqaqfzGyYGseE1zDJY+nuwvzyqrp+\nFmLUmk14/arq8VW1dVVtTTfu/XAT9zljmL+hZwFPT7Jxkk2B3YBrZzhOrdkw1/AbwO8BtPHS29NN\nBKC5b9I5jD3vmtOq6ldJjgC+BGwMnFhV1yU5rG3/56r6YpLnJLkeuB141SyGrFGGuYbAXwEPAz7S\nem/vqqpdZytm3WfI66c5asi/od9Icg6wErgHOKGqTN7niCF/B/8OOCnJCrqO2bdU1f/OWtC6V5JT\ngT2Bhyf5PnA03VC1Kecw3mFVkiRJ6gmHzUiSJEk9YfIuSZIk9YTJuyRJktQTJu+SJElST5i8S5Ik\nST1h8i5JkiT1hMm7JEmS1BP/H3xSnrUhaRiMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2ed0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='r')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\", color='g')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='b')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.76665627273672177, 0.77084632979829482, 0.7675192347681431, 0.76919076346993964, 0.77023933402705513, 0.76982310093652451]\n",
      " Mean accuracy with PCA + Logistic Regression t Classifier 0.7690\n"
     ]
    }
   ],
   "source": [
    "logr_pca_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('reduce_dim',PCA(copy=True,n_components=50,whiten=False)),        \n",
    "                     ('clf',LogisticRegression(multi_class='ovr',random_state=42,penalty='l1', tol=.01,C=1e5))])\n",
    "scores = kfold_pipeline_classifier(logr_pca_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with PCA + Logistic Regression t Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.77258081280532165, 0.77739654813890624, 0.77708463297982944, 0.77397545246515498, 0.77471383975026009, 0.77471383975026009]\n",
      " Mean accuracy with PCA + Multinomial NB Regression t Classifier 0.7751\n"
     ]
    }
   ],
   "source": [
    "mnb_pca_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    # ('reduce_dim',PCA(copy=True,n_components=10,whiten=False)),        \n",
    "                     ('clf',MultinomialNB(alpha=0.01))])\n",
    "scores = kfold_pipeline_classifier(mnb_pca_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with PCA + Multinomial NB Regression t Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
