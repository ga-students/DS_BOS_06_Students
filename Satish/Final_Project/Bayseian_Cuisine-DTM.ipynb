{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'satish'\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Data\n",
    "Using Pandas read csv method to read the data. Since there are no header columns , we read in the data as is and then later split the data using a tab as the delimiter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 col\n",
       "0  Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...\n",
       "1  Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...\n",
       "2  Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...\n",
       "3  Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...\n",
       "4  Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epic_df = pd.read_csv('Project_Cuisine/epic_recipes.txt',names=['col'],header=None)\n",
    "epic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Cleaning \n",
    "\n",
    "Since there are 3 files to read in, writing a generic method that reads in the file using pandas, cleans it up, and splits based on the tab and creates a data frame to return. Method returns a list of data frames for the lsit of files passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(filenames):\n",
    "    dfs=[]\n",
    "    for filename in filenames:\n",
    "        epic_df = pd.read_csv(filename,names=['col'],header=None)\n",
    "        epic_df['cuisine']=epic_df['col'].apply(lambda x : x.split('\\t')[0])\n",
    "        epic_df['ingredients'] = epic_df['col'].apply(lambda x:(',').join (x.split('\\t')[1:]))\n",
    "        epic_df.drop('col',inplace=True,axis=1)\n",
    "        dfs.append(epic_df)\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Load Data\n",
    "\n",
    "Loading data from all three files into three different data frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13408 entries, 0 to 13407\n",
      "Data columns (total 2 columns):\n",
      "cuisine        13408 non-null object\n",
      "ingredients    13408 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 314.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41825 entries, 0 to 41824\n",
      "Data columns (total 2 columns):\n",
      "cuisine        41825 non-null object\n",
      "ingredients    41825 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 980.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458 entries, 0 to 2457\n",
      "Data columns (total 2 columns):\n",
      "cuisine        2458 non-null object\n",
      "ingredients    2458 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 57.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "datas= clean_data(['Project_Cuisine/epic_recipes.txt','Project_Cuisine/allr_recipes.txt','Project_Cuisine/menu_recipes.txt'])\n",
    "for data in datas:\n",
    "    print data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create the uber data set\n",
    "Concatenate the three data frames created from each of the file above into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_df= pd.concat(datas)\n",
    "# Import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "# Split the data into a 70/30 train/test split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(big_df,big_df.cuisine, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_cuisine_names(cuisine):\n",
    "    return {\n",
    "        'italian':'Italian',\n",
    "        'asian':'Asian',\n",
    "        'mexico':'Mexico',\n",
    "        'japanese':'Japanese',\n",
    "        'chinese':'Chinese',\n",
    "        'China'  :'Chinese', \n",
    "        'korean' : 'Korean',\n",
    "        'Japan':'Japanese',\n",
    "        'Korea':'Korean',\n",
    "        'France' :'French',\n",
    "        'India'  :'Indian',\n",
    "        'Italy'  :'Italian',\n",
    "        'Thailand' :'Thai',\n",
    "        'Mexico':'Mexican',\n",
    "        'Scandinavia':'Scandinavian',\n",
    "        'Germany':'German'\n",
    "        \n",
    "    }.get(cuisine,cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_df.cuisine= big_df.cuisine.apply(lambda x : standardize_cuisine_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_dict = {}\n",
    "with open('Project_Cuisine/map.txt') as f:\n",
    "    for line in f:\n",
    "        keys = line.split()\n",
    "        if(len(keys)>1):\n",
    "            (key,val)=keys\n",
    "            map_dict[key]=val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_df['cuisine_group']= big_df.cuisine.apply(lambda x : map_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>cuisine_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vinegar,cilantro,mint,olive_oil,cayenne,fish,l...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>onion,cayenne,fish,black_pepper,seed,garlic</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>garlic,soy_sauce,lime_juice,thai_pepper</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cuisine                                        ingredients  \\\n",
       "0  Vietnamese  vinegar,cilantro,mint,olive_oil,cayenne,fish,l...   \n",
       "1  Vietnamese        onion,cayenne,fish,black_pepper,seed,garlic   \n",
       "2  Vietnamese            garlic,soy_sauce,lime_juice,thai_pepper   \n",
       "\n",
       "    cuisine_group  \n",
       "0  SoutheastAsian  \n",
       "1  SoutheastAsian  \n",
       "2  SoutheastAsian  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(big_df.ingredients, big_df.cuisine_group, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vectorization\n",
    "The features in this data set are the ingredients of the dish themselves. To build the feature vectors, we need to vectorize these words/features into a feature vector. Since this is text, we use a count vectorizer to do that. Now in this case since an ingredient will either occur or not in a dish ,this fits the Boolean Occurence model of the Bag of Words model. The appropriate vectorizer to choose in this case would be the Binary Vectorizer..essentially setting the binary property of CountVectorizer to true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create a TFIDF vectorizer\n",
    "count_vect = CountVectorizer(decode_error = 'ignore', binary=True)\n",
    "# Call fit to do our frequency vectorization\n",
    "count_vect.fit(X_train)\n",
    "train_simple_dtm = count_vect.transform(X_train)\n",
    "test_simple_dtm = count_vect.transform(X_test)\n",
    "\n",
    "##Lets see how the prediction works for the cuisine itself \n",
    "unique_cuisine_group = big_df.cuisine_group.unique().tolist()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "le.fit(unique_cuisine_group)\n",
    "y_train =  le.transform(y_train)\n",
    "y_test =  le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature Vector Array\n",
    "We now convert the feature vector array to a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68679223480471463"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# Create the model\n",
    "bnb = BernoulliNB()\n",
    "# Fit the model to the training data\n",
    "bnb.fit(train_simple_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "bnb.score(test_simple_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7394268546336954"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ensemble_r = RandomForestClassifier()\n",
    "ensemble_r.fit(train_simple_dtm,y_train)\n",
    "#y_pred = ensemble_r.predict(test_simple_dtm)\n",
    "ensemble_r.score(test_simple_dtm,y_test)\n",
    "\n",
    "#print cross_val_score(ensemble_r,test_simple_dtm,y_cuisine).mean()\n",
    "#confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739484631384\n",
      "0.737751328865\n",
      "0.738964640629\n"
     ]
    }
   ],
   "source": [
    "ensemble_d = DecisionTreeClassifier(max_depth=10,min_samples_split=1)\n",
    "ensemble_d.fit(train_simple_dtm,y_train)\n",
    "print ensemble_d.score(test_simple_dtm,y_test)\n",
    "\n",
    "\n",
    "ensemble_r = RandomForestClassifier(n_estimators=10)\n",
    "ensemble_r.fit(train_simple_dtm,y_train)\n",
    "print ensemble_r.score(test_simple_dtm,y_test)\n",
    "\n",
    "\n",
    "ensemble_e = ExtraTreesClassifier(n_estimators=10)\n",
    "ensemble_e.fit(train_simple_dtm,y_train)\n",
    "print ensemble_e.score(test_simple_dtm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Feature Engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim',PCA(copy=True,n_components=10,whiten=False)),('svm',SVC())]\n",
    "clf = Pipeline(estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=10, whiten=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_simple_dtm.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73388028657268312"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_simple_dtm.toarray(),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 50 eigenfaces from 377 faces\n",
      "done in 1.087s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "\n",
    "# Compute a PCA (eigenfaces) on the face dataset\n",
    "# Keep 50 principal components\n",
    "n_components = 50\n",
    "\n",
    "print \"Extracting the top %d eigenfaces from %d faces\" % (\n",
    "    n_components, train_simple_dtm.toarray().shape[1])\n",
    "t0 = time()\n",
    "# Create a RandomizedPCA with n_components and fit it to X_train\n",
    "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(train_simple_dtm.toarray())\n",
    "print \"done in %0.3fs\" % (time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.261s\n"
     ]
    }
   ],
   "source": [
    "print \"Projecting the input data on the eigenfaces orthonormal basis\"\n",
    "t0 = time()\n",
    "# Transform X_train to X_train_pca\n",
    "X_train_pca = pca.transform(train_simple_dtm.toarray())\n",
    "# Transform X_test to X_test_pca\n",
    "X_test_pca = pca.transform(test_simple_dtm.toarray())\n",
    "print \"done in %0.3fs\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a param_grid for GridSearchCV\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a SVM classification model\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "# Create GridSearchCV\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
    "# Fit the model\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print \"Predicting the people names on the testing set\"\n",
    "t0 = time()\n",
    "# Generate test predictions as y_pred\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "# Print classification_report\n",
    "print classification_report(y_test, y_pred, target_names=target_names)\n",
    "# Print confusion_matrix\n",
    "print confusion_matrix(y_test, y_pred, labels=range(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
