{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cuisine Prediction Based on Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspiration for this machine learning project comes from two things I love- food and economics,in that order. Or specifically from this article called [What Are the Defining Ingredients of a Culture’s Cuisine?](http://priceonomics.com/what-are-the-defining-ingredients-of-a-cultures/).  As Dan argues in the article, every cuisine (or most of them) seem to have some specific ingredients that underpin the most if not all dishes from that cusines. See [here](https://tableagent.com/article/an-overview-of-indias-regional-cuisines/), [here](http://www.splendidtable.org/story/the-very-definition-of-mexican-food-is-a-multicultural-cuisine) and [here](http://www.travelchinaguide.com/intro/cuisine_drink/cuisine/) for some examples of what I mean by the above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, what interesting from a data perspective is that, having certain defining ingredient(s) simply translates to saying there are certain specific axes or vectors  which a prediction algorithm ought to be able to identify that helps you predict with some accuracy that a dish belongs to a certain cuisine given its ingredients.\n",
    "\n",
    "[Prof.Yong-Yeol “YY” Ahn](http://yongyeol.com/) of Indian University,Bloomington provides such a [dataset](http://yongyeol.com/data/). The dataset accumulated from various recipes from magazines like [Epicurious](http://www.epicurious.com/),[Gourmet](http://www.gourmet.com/) and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the usual tools\n",
    "__author__ = 'satish'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "#Set seed to replicate results.\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Read Data\n",
    "Using Pandas read csv method to read the data. Since there are no header columns , we read in the data as is and then later split the data using a tab as the delimiter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 col\n",
       "0  Vietnamese\\tvinegar\\tcilantro\\tmint\\tolive_oil...\n",
       "1  Vietnamese\\tonion\\tcayenne\\tfish\\tblack_pepper...\n",
       "2  Vietnamese\\tgarlic\\tsoy_sauce\\tlime_juice\\ttha...\n",
       "3  Vietnamese\\tcilantro\\tshallot\\tlime_juice\\tfis...\n",
       "4  Vietnamese\\tcoriander\\tvinegar\\tlemon\\tlime_ju..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epic_df = pd.read_csv('data/epic_recipes.txt',names=['col'],header=None)\n",
    "epic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the data above, the data is not clearly tabulated to create a useable dataframe. Some amount of data munging would be need to done before the data is in a usable format. First thing we need to do is to split the 'col' column into Cuisine and the corresponding ingredients. The first element when split by '\\t' we get is the cuisine. Further the rest of the columns would be split by '\\t' and then joined by ',' and reset into the dataframe for pandas to parse it correctly. Since there are 3 files in all to parse, we encapsulate the clean up and parsing logic into a simple function that we apply on all data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(filenames):\n",
    "    dfs=[]\n",
    "    for filename in filenames:\n",
    "        epic_df = pd.read_csv(filename,names=['col'],header=None)\n",
    "        epic_df['cuisine']=epic_df['col'].apply(lambda x : x.split('\\t')[0])\n",
    "        epic_df['ingredients'] = epic_df['col'].apply(lambda x:(',').join (x.split('\\t')[1:]))\n",
    "        epic_df.drop('col',inplace=True,axis=1)\n",
    "        dfs.append(epic_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13408 entries, 0 to 13407\n",
      "Data columns (total 2 columns):\n",
      "cuisine        13408 non-null object\n",
      "ingredients    13408 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 314.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41825 entries, 0 to 41824\n",
      "Data columns (total 2 columns):\n",
      "cuisine        41825 non-null object\n",
      "ingredients    41825 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 980.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2458 entries, 0 to 2457\n",
      "Data columns (total 2 columns):\n",
      "cuisine        2458 non-null object\n",
      "ingredients    2458 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 57.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chinese</td>\n",
       "      <td>onion,beef,starch,sake,soy_sauce,scallion,lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chinese</td>\n",
       "      <td>pork,onion,black_pepper,cayenne,scallion,bean,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese</td>\n",
       "      <td>tomato,vinegar,celery_oil,onion,corn,cayenne,g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinese</td>\n",
       "      <td>wheat,sesame_oil,starch,sake,soy_sauce,cayenne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cuisine                                        ingredients\n",
       "0  chinese  onion,beef,starch,sake,soy_sauce,scallion,lett...\n",
       "1  chinese  pork,onion,black_pepper,cayenne,scallion,bean,...\n",
       "2  chinese  tomato,vinegar,celery_oil,onion,corn,cayenne,g...\n",
       "3  chinese  wheat,sesame_oil,starch,sake,soy_sauce,cayenne..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas= clean_data(['data/epic_recipes.txt','data/allr_recipes.txt','data/menu_recipes.txt'])\n",
    "for data in datas:\n",
    "    print data.info()\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we end up with 3 dataframes each with two columns - cuisine and ingredients,each with the cuisine seperated from rest of the ingredients which are left as comma seperated values in the dataframe. We then join the three seperate dataframes into a single dataframe by concatenating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vinegar,cilantro,mint,olive_oil,cayenne,fish,l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>onion,cayenne,fish,black_pepper,seed,garlic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>garlic,soy_sauce,lime_juice,thai_pepper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cuisine                                        ingredients\n",
       "0  Vietnamese  vinegar,cilantro,mint,olive_oil,cayenne,fish,l...\n",
       "1  Vietnamese        onion,cayenne,fish,black_pepper,seed,garlic\n",
       "2  Vietnamese            garlic,soy_sauce,lime_juice,thai_pepper"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(datas)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Standardization\n",
    "Now that we have a concatenated set of dataframes, lets for a unique data uniqueness issues. Printing the 15 of sorted list of  cuisines reveal issues such as spelling mistakes, duplicate classification (china,chinese) and other errors. We run a simple standardization method on all cuisine column values to fix those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'american',\n",
       " 'asian',\n",
       " 'asian',\n",
       " 'austria',\n",
       " 'bangladesh',\n",
       " 'belgium',\n",
       " 'cajun_creole',\n",
       " 'canada',\n",
       " 'caribbean',\n",
       " 'central_southamerican',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chinese',\n",
       " 'east-african']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([x.lower() for x in df.cuisine.unique().tolist()])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_cuisine_names(cuisine):\n",
    "    return {\n",
    "        'italian':'Italian',\n",
    "        'asian':'Asian',\n",
    "        'mexico':'Mexico',\n",
    "        'japanese':'Japanese',\n",
    "        'chinese':'Chinese',\n",
    "        'China'  :'Chinese', \n",
    "        'korean' : 'Korean',\n",
    "        'Japan':'Japanese',\n",
    "        'Korea':'Korean',\n",
    "        'France' :'French',\n",
    "        'India'  :'Indian',\n",
    "        'Italy'  :'Italian',\n",
    "        'Thailand' :'Thai',\n",
    "        'Mexico':'Mexican',\n",
    "        'Scandinavia':'Scandinavian',\n",
    "        'Germany':'German'\n",
    "        \n",
    "    }.get(cuisine,cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.cuisine= df.cuisine.apply(lambda x : standardize_cuisine_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'american',\n",
       " 'asian',\n",
       " 'austria',\n",
       " 'bangladesh',\n",
       " 'belgium',\n",
       " 'cajun_creole',\n",
       " 'canada',\n",
       " 'caribbean',\n",
       " 'central_southamerican',\n",
       " 'chinese',\n",
       " 'east-african',\n",
       " 'east_asian',\n",
       " 'eastern-europe',\n",
       " 'easterneuropean_russian']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([x.lower() for x in df.cuisine.unique().tolist()])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another level of classification would be to move from a simple country specific cuisine to a regional specific cuisine. This will help us move away from issues such eastern_europe and easterneurpoean_russian. Fortunately the map.txt in the dataset already classifies all cuisines to each of the region specific. We read this mapping - country specific cuisine to a particular region mapping into dict to make the look ups simple. The we use the apply function in pandas to apply the lambda function to look up the region for each of the cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_dict = {}\n",
    "with open('data/map.txt') as f:\n",
    "    for line in f:\n",
    "        keys = line.split()\n",
    "        if(len(keys)>1):\n",
    "            (key,val)=keys\n",
    "            map_dict[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cuisine_group']= df.cuisine.apply(lambda x : map_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>cuisine_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>vinegar,cilantro,mint,olive_oil,cayenne,fish,l...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>onion,cayenne,fish,black_pepper,seed,garlic</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>garlic,soy_sauce,lime_juice,thai_pepper</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>cilantro,shallot,lime_juice,fish,cayenne,ginge...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>coriander,vinegar,lemon,lime_juice,fish,cayenn...</td>\n",
       "      <td>SoutheastAsian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cuisine                                        ingredients  \\\n",
       "0  Vietnamese  vinegar,cilantro,mint,olive_oil,cayenne,fish,l...   \n",
       "1  Vietnamese        onion,cayenne,fish,black_pepper,seed,garlic   \n",
       "2  Vietnamese            garlic,soy_sauce,lime_juice,thai_pepper   \n",
       "3  Vietnamese  cilantro,shallot,lime_juice,fish,cayenne,ginge...   \n",
       "4  Vietnamese  coriander,vinegar,lemon,lime_juice,fish,cayenn...   \n",
       "\n",
       "    cuisine_group  \n",
       "0  SoutheastAsian  \n",
       "1  SoutheastAsian  \n",
       "2  SoutheastAsian  \n",
       "3  SoutheastAsian  \n",
       "4  SoutheastAsian  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Exploration.\n",
    "Lets look at some patterns in the data before we try our hand aat any of the prediction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorthAmerican       41524\n",
      "SouthernEuropean     4180\n",
      "LatinAmerican        2917\n",
      "WesternEuropean      2659\n",
      "EastAsian            1713\n",
      "MiddleEastern         645\n",
      "SouthAsian            621\n",
      "SoutheastAsian        457\n",
      "EasternEuropean       381\n",
      "African               352\n",
      "NorthernEuropean      250\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10b0b8190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFhCAYAAACYmKqMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109eebc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Examine the cuisine groups. \n",
    "print df.cuisine_group.value_counts()\n",
    "df.groupby('cuisine_group')['cuisine'].count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly no suprise there. As most of the recipes came from American Magazines,North American cuisine is defintely overreprested as compared to the other regions, followed by Southern European and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Vectorization\n",
    "The features in this data set are the ingredients of the dish themselves.Each ingredient specific to that dish becomes a feature of that dish and hence the cuisine.Since this is text, we use a count vectorizer to do that. Now in this case since an ingredient will either occur or not in a dish ,this fits the Boolean Occurence model of the Bag of Words model. The appropriate vectorizer to choose in this case would be the Binary Vectorizer..essentially setting the binary property of CountVectorizer to true.\n",
    "Once we vectorize using a binary vectorizer, we build a document term matrix using the ingredients of each recipe as the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] [u'almond', u'angelica', u'anise', u'anise_seed', u'apple', u'apple_brandy', u'apricot', u'armagnac', u'artemisia', u'artichoke', u'asparagus', u'avocado', u'bacon', u'baked_potato', u'balm', u'banana', u'barley', u'bartlett_pear', u'basil', u'bay', u'bean', u'beech', u'beef', u'beef_broth', u'beef_liver', u'beer', u'beet', u'bell_pepper', u'bergamot', u'berry', u'bitter_orange', u'black_bean', u'black_currant', u'black_mustard_seed_oil', u'black_pepper', u'black_raspberry', u'black_sesame_seed', u'black_tea', u'blackberry', u'blackberry_brandy', u'blue_cheese', u'blueberry', u'bone_oil', u'bourbon_whiskey', u'brandy', u'brassica', u'bread', u'broccoli', u'brown_rice', u'brussels_sprout', u'buckwheat', u'butter', u'buttermilk', u'cabbage', u'cabernet_sauvignon_wine', u'cacao', u'camembert_cheese', u'cane_molasses', u'caraway', u'cardamom', u'carnation', u'carob', u'carrot', u'cashew', u'cassava', u'catfish', u'cauliflower', u'caviar', u'cayenne', u'celery', u'celery_oil', u'cereal', u'chamomile', u'champagne_wine', u'chayote', u'cheddar_cheese', u'cheese', u'cherry', u'cherry_brandy', u'chervil', u'chicken', u'chicken_broth', u'chicken_liver', u'chickpea', u'chicory', u'chinese_cabbage', u'chive', u'cider', u'cilantro', u'cinnamon', u'citrus', u'citrus_peel', u'clam', u'clove', u'cocoa', u'coconut', u'coconut_oil', u'cod', u'coffee', u'cognac', u'concord_grape', u'condiment', u'coriander', u'corn', u'corn_flake', u'corn_grit', u'cottage_cheese', u'crab', u'cranberry', u'cream', u'cream_cheese', u'cucumber', u'cumin', u'cured_pork', u'currant', u'date', u'dill', u'durian', u'eel', u'egg', u'egg_noodle', u'elderberry', u'emmental_cheese', u'endive', u'enokidake', u'fennel', u'fenugreek', u'feta_cheese', u'fig', u'fish', u'flower', u'frankfurter', u'fruit', u'galanga', u'gardenia', u'garlic', u'gelatin', u'geranium', u'gin', u'ginger', u'goat_cheese', u'grape', u'grape_brandy', u'grape_juice', u'grapefruit', u'green_bell_pepper', u'green_tea', u'gruyere_cheese', u'guava', u'haddock', u'ham', u'hazelnut', u'herring', u'holy_basil', u'honey', u'hop', u'horseradish', u'huckleberry', u'jamaican_rum', u'japanese_plum', u'jasmine', u'jasmine_tea', u'juniper_berry', u'kaffir_lime', u'kale', u'katsuobushi', u'kelp', u'kidney_bean', u'kiwi', u'kohlrabi', u'kumquat', u'lamb', u'lard', u'laurel', u'lavender', u'leaf', u'leek', u'lemon', u'lemon_juice', u'lemon_peel', u'lemongrass', u'lentil', u'lettuce', u'licorice', u'lilac_flower_oil', u'lima_bean', u'lime', u'lime_juice', u'lime_peel_oil', u'lingonberry', u'litchi', u'liver', u'lobster', u'long_pepper', u'lovage', u'macadamia_nut', u'macaroni', u'mace', u'mackerel', u'malt', u'mandarin', u'mandarin_peel', u'mango', u'maple_syrup', u'marjoram', u'mate', u'matsutake', u'meat', u'melon', u'milk', u'milk_fat', u'mint', u'mozzarella_cheese', u'mung_bean', u'munster_cheese', u'muscat_grape', u'mushroom', u'mussel', u'mustard', u'mutton', u'nectarine', u'nira', u'nut', u'nutmeg', u'oat', u'oatmeal', u'octopus', u'okra', u'olive', u'olive_oil', u'onion', u'orange', u'orange_flower', u'orange_juice', u'orange_peel', u'oregano', u'ouzo', u'oyster', u'palm', u'papaya', u'parmesan_cheese', u'parsley', u'parsnip', u'passion_fruit', u'pea', u'peach', u'peanut', u'peanut_butter', u'peanut_oil', u'pear', u'pear_brandy', u'pecan', u'pelargonium', u'pepper', u'peppermint', u'peppermint_oil', u'pimenta', u'pimento', u'pineapple', u'pistachio', u'plum', u'popcorn', u'porcini', u'pork', u'pork_liver', u'pork_sausage', u'port_wine', u'potato', u'potato_chip', u'prawn', u'prickly_pear', u'provolone_cheese', u'pumpkin', u'quince', u'radish', u'raisin', u'rapeseed', u'raspberry', u'raw_beef', u'red_algae', u'red_bean', u'red_kidney_bean', u'red_wine', u'rhubarb', u'rice', u'roasted_almond', u'roasted_beef', u'roasted_hazelnut', u'roasted_meat', u'roasted_nut', u'roasted_peanut', u'roasted_pecan', u'roasted_pork', u'roasted_sesame_seed', u'romano_cheese', u'root', u'roquefort_cheese', u'rose', u'rosemary', u'rum', u'rutabaga', u'rye_bread', u'rye_flour', u'saffron', u'sage', u'sake', u'salmon', u'salmon_roe', u'sassafras', u'sauerkraut', u'savory', u'scallion', u'scallop', u'sea_algae', u'seaweed', u'seed', u'sesame_oil', u'sesame_seed', u'shallot', u'sheep_cheese', u'shellfish', u'sherry', u'shiitake', u'shrimp', u'smoke', u'smoked_fish', u'smoked_salmon', u'smoked_sausage', u'sour_cherry', u'sour_milk', u'soy_sauce', u'soybean', u'soybean_oil', u'spearmint', u'squash', u'squid', u'star_anise', u'starch', u'strawberry', u'strawberry_jam', u'strawberry_juice', u'sturgeon_caviar', u'sumac', u'sunflower_oil', u'sweet_potato', u'swiss_cheese', u'tabasco_pepper', u'tamarind', u'tangerine', u'tarragon', u'tea', u'tequila', u'thai_pepper', u'thyme', u'tomato', u'tomato_juice', u'truffle', u'tuna', u'turkey', u'turmeric', u'turnip', u'vanilla', u'veal', u'vegetable', u'vegetable_oil', u'vinegar', u'violet', u'walnut', u'wasabi', u'watercress', u'watermelon', u'wheat', u'wheat_bread', u'whiskey', u'white_bread', u'white_wine', u'whole_grain_wheat_flour', u'wine', u'wood', u'yam', u'yeast', u'yogurt', u'zucchini']\n"
     ]
    }
   ],
   "source": [
    "# Create a TFIDF vectorizer\n",
    "count_vect = CountVectorizer(decode_error = 'ignore', binary=True)\n",
    "# Call fit to do our frequency vectorization\n",
    "df_dtm =  count_vect.fit_transform(df.ingredients)\n",
    "print df_dtm.toarray(),count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####TBD - Look for patterns such as what ingredients occur most across all cuisines, across each cuisine, whats most rarest among all cuisines, among each cuisine. Types of meats and other specific ingredients across each cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets fit some models.   - Round 1 - Stick to Standard Text Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, 'African', 'EastAsian', 'EasternEuropean', 'LatinAmerican',\n",
       "       'MiddleEastern', 'NorthAmerican', 'NorthernEuropean', 'SouthAsian',\n",
       "       'SoutheastAsian', 'SouthernEuropean', 'WesternEuropean'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Before any thing, lets split the entire data set into training and test. \n",
    "# Split the data into a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.ingredients, df.cuisine_group, test_size=0.3)\n",
    "unique_cuisine_group = df.cuisine_group.unique().tolist()\n",
    "##For us to be able to use any of the prediction models we need turn the cuisine groups from earlier into numerical labels.\n",
    "## We use the standard Label Encoder to this.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "le.fit(unique_cuisine_group)\n",
    "y_train =  le.transform(y_train)\n",
    "y_test =  le.transform(y_test)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68835220707187428"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Build the Document Term Matrix based on the vectorizer that was created earler. \n",
    "train_simple_dtm = count_vect.transform(X_train)\n",
    "test_simple_dtm = count_vect.transform(X_test)\n",
    "\n",
    "##Given that we have a set of feature vectors that have a binary occurence probability in a certain dish/document, we \n",
    "#look at the naive bayseien model as our initial model. \n",
    "# Create the model\n",
    "bnb = BernoulliNB()\n",
    "# Fit the model to the training data\n",
    "bnb.fit(train_simple_dtm, y_train)\n",
    "# Score the model against the test data\n",
    "bnb.score(test_simple_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a simple Bernoulli Naive Bayes model with no frills is able to predict with about 69.3% accuracy on our test data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up a pipeline for vectorization , transformation and classification\n",
    "classfication with a Multinomial Bayesian classifier while also looking building a TF-IDF Transformer\n",
    "\n",
    "\n",
    "I use a stratified K fold classifier to ensure I have equally weighted sample of different classes in my train and test samples. I do this because as I showed earlier I have lop-sided percentage of various types of my cuisine groups in my data set. \n",
    "Similarly I set up a pipe line to build a document term matrix for each train sample using a binary vectorizer with a TF-IDF document transformer.\n",
    "\n",
    "Similarly I use a simple accuracy score function to measure the accuracy of the prediction. I also set up methods to generate a confusion matrix and plotting of a confusion matrix for on hand later.\n",
    "\n",
    "###QQQ - Is a simple accuracy score a good measure of accuracy in this case, or should I also weight the accuracy based on the class i.e. use f1-score with average = weighted ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from  sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, df,title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df.cuisine_group))\n",
    "    plt.xticks(tick_marks, df.cuisine_group, rotation=45)\n",
    "    plt.yticks(tick_marks, df.cuisine_group)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def compute_cm(y_test,y_pred,df):\n",
    "    cm = confusion_matrix(test_y,predictions)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plot_confusion_matrix(cm,data)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized)\n",
    "    plot_confusion_matrix(cm_normalized,data, title='Normalized confusion matrix')\n",
    "    \n",
    "\n",
    "    \n",
    "def kfold_pipeline_classifier(pipeline,data):\n",
    "    #sss = cross_validation.StratifiedShuffleSplit(le.transform(data.cuisine_group),test_size =0.3,random_state=42)\n",
    "    sss = cross_validation.StratifiedKFold(le.transform(data.cuisine_group),n_folds=6,shuffle=True,random_state=42)\n",
    "    scores = []\n",
    "    confusion = np.array([[0, 0], [0, 0]])\n",
    "    for train_indices, test_indices in sss:\n",
    "        train_text = data.iloc[train_indices]['ingredients'].values\n",
    "        train_y = le.transform(data.iloc[train_indices]['cuisine_group'].values)\n",
    "        test_text = data.iloc[test_indices]['ingredients'].values\n",
    "        test_y = le.transform(data.iloc[test_indices]['cuisine_group'].values)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "        #score = f1_score(test_y, predictions, average='weighted') \n",
    "        score =accuracy_score(test_y,predictions)\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.68402452967466998, 0.68267831149927216, 0.6876689540444999, 0.6949240690659455, 0.6918834547346514, 0.68126951092611865]\n",
      " Mean accuracy with BernoulliNB 0.6871\n"
     ]
    }
   ],
   "source": [
    "bernouli_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',BernoulliNB())])\n",
    "scores = kfold_pipeline_classifier(bernouli_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with BernoulliNB %.4f\" % np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.75044174202265879, 0.7538989394884591, 0.75556248700353501, 0.75722904098190136, 0.75764828303850151, 0.7513007284079084]\n",
      " Mean accuracy with MultinomialNB 0.7543\n"
     ]
    }
   ],
   "source": [
    "multinomial_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',MultinomialNB())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(multinomial_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with MultinomialNB %.4f\" % np.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, lets also look at the commonly used classifiers for text classification before moving to feature engineering or grid searches for optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.058517825589855521, 0.051777916406737366, 0.057288417550426282, 0.05627210318285833, 0.054110301768990635, 0.050572320499479712]\n",
      " Mean accuracy with GaussianNB 0.0548\n"
     ]
    }
   ],
   "source": [
    "guassian_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',GaussianNB())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(guassian_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with GaussianNB %.4f\" % np.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logistic Regression \n",
    "Another workhorse of text classification ,trying the Logistic regression for multi class set up case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.76582475834112873, 0.76803909336660425, 0.76959866916198794, 0.76503016434366544, 0.76847034339229969, 0.76555671175858486]\n",
      " Mean accuracy with Logistic regression 0.7671\n"
     ]
    }
   ],
   "source": [
    "logistic_regr_pipeline_l2 = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',LogisticRegression(multi_class='ovr',random_state=42,penalty='l2', tol=.01,C=1e5))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(logistic_regr_pipeline_l2,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Logistic regression %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.76457748674773929, 0.76835100852568106, 0.76897483884383444, 0.76627834408154771, 0.77013527575442242, 0.76701352757544228]\n",
      " Mean accuracy with Logistic regression 0.7676\n"
     ]
    }
   ],
   "source": [
    "logistic_regr_pipeline_l1 = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf',LogisticRegression(multi_class='ovr',random_state=42,penalty='l1', tol=.01,C=1e5))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(logistic_regr_pipeline_l1,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Logistic regression %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Classifiers.\n",
    "So we achived a 75.5% accuracy on the model with the Multinomial NB and about 76.7% with the Logistic Regression for multi class classification. Lets see if we can do better with a more sophisticated algorithm such a support vector machines which is generally regarded as on of the best text classification algorithms. These are specially effective in high dimensional space as is the case here. Running the standard SVM classifiers to see the prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.73266812181685892, 0.7327926803909337, 0.73331253898939486, 0.73184938631162888, 0.73163371488033302, 0.73111342351716957]\n",
      " Mean accuracy with SGDClassifier 0.7322\n"
     ]
    }
   ],
   "source": [
    "sgd_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(sgd_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with SGDClassifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.73921629768215358, 0.66957787481804953, 0.73445622790600962, 0.69221967963386732, 0.73610822060353798, 0.69334027055150882]\n",
      " Mean accuracy with Linear SVC 0.7108\n"
     ]
    }
   ],
   "source": [
    "linear_svc_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',LinearSVC(penalty='l2',tol=.01,C=1e5,multi_class='ovr'))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(linear_svc_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Linear SVC %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initial GridSearch with MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  36 | elapsed:   11.0s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 13.2120549679s\n",
      "Best score: 0.756184533096\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\ttfidf__norm: l2\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "# Create GridSearchCV\n",
    "pipeline = Pipeline(\n",
    "    [('vec', CountVectorizer(encoding='cp874', decode_error='ignore', binary=True)), ('tfidf', TfidfTransformer(sublinear_tf=True,use_idf=True,)),\n",
    "     ('clf', MultinomialNB())])\n",
    "parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in {0}s\".format(time() - t0))\n",
    "print(\"Best score: {0}\".format(grid_search.best_score_))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(list(parameters.keys())):\n",
    "    print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run a few ensemble classifiers before attempting Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.7439975054568132, 0.74007070076939074, 0.74474942815554168, 0.74308300395256921, 0.73995837669094688, 0.74349635796045788]\n",
      " Mean accuracy with Random Forest Classifier 0.7426\n"
     ]
    }
   ],
   "source": [
    "## RandomForest Classifier\n",
    "\n",
    "rf_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',RandomForestClassifier())])\n",
    "\n",
    "scores = kfold_pipeline_classifier(rf_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.7376572081904168, 0.73778332293616133, 0.74599708879184867, 0.7402745995423341, 0.74266389177939651, 0.74099895941727367]\n",
      " Mean accuracy with Random Forest Classifier 0.7409\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',DecisionTreeClassifier(max_depth=10,min_samples_split=1))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(dt_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores \n",
      "[0.74420538405571146, 0.74111041796631316, 0.74807652318569351, 0.74578739338464739, 0.74807492195629555, 0.74557752341311134]\n",
      " Mean accuracy with Random Forest Classifier 0.7455\n"
     ]
    }
   ],
   "source": [
    "## Extra Trees Classifier\n",
    "et_pipeline = Pipeline([('vect',CountVectorizer(decode_error = 'ignore', binary=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                    ('clf',ExtraTreesClassifier(n_estimators=10))])\n",
    "\n",
    "scores = kfold_pipeline_classifier(et_pipeline,df)\n",
    "print \"Cross Validation Scores \"\n",
    "print scores\n",
    "print \" Mean accuracy with Random Forest Classifier %.4f\" % np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
